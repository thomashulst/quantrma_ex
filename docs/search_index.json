[["index.html", "Lab manual QuantRMA Preface Important notes", " Lab manual QuantRMA Original authors: Matthew J. C. Crump, Anjali Krishnan, Stephen Volz, and Alla Chavarga Adapted for EUC by Thomas Hulst and Thanos Kostopoulos Last Compiled 2021-03-19 Preface These course materials were used during academic year 2020/2021. The materials will stay online for the foreseeable future, but will no longer be updated. A new course website will be created for academic year 2021/2022. Important notes This is the lab manual for Quantitative Research Methods &amp; Analysis at EUC. As with the textbook, this manual was adapted from “Answering questions with data” by Matthew J.C. Crump. The original text is part of a larger OER (Open Educational Resource) course for teaching undergraduate statistics in psychology. As such, the text assumes you are a psychology student and many of the examples are drawn from the field of psychology. This does not mean that this course is only useful for you if you have an interest in psychology. The field of psychology will serve as a vehicle to teach you important concepts and skills in quantitative research methods and data analysis, but the concepts and skills taught are universal. This manual provides the exercises we will work on during labs. We use open-data sets that are usually paired with a primary research article. The manual is a free and open resource. See below for more information about copying, making change, or contributing to the lab manual. Attributions The original lab manual was authored by Matthew Crump with exercises adapted and expanded from Open Stats Labs. CC BY-SA 4.0 license All resources are released under a creative commons licence CC BY-SA 4.0. Click the link to read more about the license, or read more below: This license means that you are free to: Share: copy and redistribute the material in any medium or format Adapt: remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlike: If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions: You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. "],["getting-started-adapted1.html", "Getting started1 0.1 Why R? 0.2 Downloading and installing R 0.3 Downloading and installing RStudio 0.4 Understanding RStudio 0.5 How to complete the labs", " Getting started1 In this course we will be using R as a tool to analyze data, and as a tool to help us gain a better understanding of what our analyses are doing. Throughout each lab we will show you how to use R to solve specific problems, and then you will use the examples to solve assignments. R is a very deep programming language, and in many ways we will only be skimming the surface of what R can do. Along the way, there will be many pointers to more advanced techniques that interested students can follow to become experts in using R for data-analysis, and computer programming in general. R is primarily a computer programming language for statistical analysis. It is free, and open-source (i.e. many people contribute to developing it), and runs on most operating systems. It is a powerful language that can be used for all sorts of mathematical operations, data-processing, analysis, and graphical display of data. I even used R to write this lab manual. And, I use R all the time for my own research, because it makes data-analysis fast, efficient, transparent and reproducible. 0.1 Why R? There are lots of different options for using computers to analyze data, so why use R?2 The options all have pros and cons, and can be used in different ways to solve a range of different problems. Some software allows you to load in data, and then analyze the data by clicking different options in a menu. This can sometimes be fast and convenient. For example, once the data is loaded, all you have to do is click a couple buttons to analyze the data! However, many aspects of data-analysis are not so easy. For example, particular analyses often require that the data be formatted in a particular way so that the program can analyze it properly. Often times when a researcher wants to ask a new question of an existing data set, they have to spend time re-formatting the data. If the data is large, then reformatting by hand is very slow, and can lead to errors. Another option, is to use a scripting language to instruct the computer how reformat the data. This is very fast and efficient. R provides the ability to do everything all in one place. You can load in data, reformat it any way you like, then analyze it anyway you like, and create beautiful graphs and tables (publication quality) to display your findings. 0.2 Downloading and installing R Okay, enough with the sales pitch. The website for downloading R is: https://cloud.r-project.org. At the top of the page – under the heading “Download and Install R” – you’ll see separate links for Mac users, Windows users, and Linux users. If you follow the relevant link, you’ll see that the online instructions are pretty self-explanatory, but I’ll walk you through the installation anyway. 0.2.1 Installing R on a Mac The CRAN (Comprehensive R Archive Network) homepage changes from time to time, and it’s not particularly pretty, or all that well-designed quite frankly. But it’s not difficult to find what you’re after. When you click on the (Mac) OS X link, you should find yourself on a page with the title “R for Mac OS X.” There’s a fairly prominent link on the page under “Latest release” called “R-x.x.x.pkg” (where the x.x.x is replaced by the most current version number), which is the one you want. Click on that link and you’ll start downloading the installer file, which is (not surprisingly) called R-x.x.x.pkg. Once you’ve downloaded R-x.x.x.pkg, all you need to do is open it by double clicking on the package file. The installation should go smoothly from there: just follow all the instructions just like you usually do when you install something. Once it’s finished, you’ll find a file called R.app in the Applications folder. You can now open up R in the usual way if you want to, but what I’m going to suggest is that instead of doing that you should now install RStudio (see this section for instructions). 0.2.2 Installing R on a Windows PC You’ll find a link at the top of the page with the text “Download R for Windows.” If you click on that, it will take you to a page that offers you a few options. Again, at the very top of the page you’ll be told to click on a link that says to click here if you’re installing R for the first time. This will take you to a page that has a prominent link at the top called “Download R x.x.x for Windows” (where the x.x.x is replaced by the most current version number). That’s the one you want. Click on that and your browser should start downloading a file called R-x.x.x-win.exe. Once you’ve downloaded the file, double click to install it. As with any software you download online, Windows will ask you some questions about whether you trust the file and so on. After you click through those, it’ll ask you where you want to install it, and what components you want to install. The default values should be fine for most people, so again, just click through. Once all that is done, you should have R installed on your system. You can access it from the Start menu, or from the desktop if you asked it to add a shortcut there. You can now open up R in the usual way if you want to, but what I’m going to suggest is that instead of doing that you should now install RStudio (see this section for instructions). 0.2.3 Installing R on a Linux PC Honestly, if you are working on a Linux PC, you can probably figure this out yourself. If you have trouble installing R on your Linux box, please see the course coordinator during office hours. Once you’ve got R installed, you can run it from the command line just by typing R. However, if you’re feeling envious of Windows and Mac users for their fancy GUIs, you can download RStudio too (see this section for instructions). 0.3 Downloading and installing RStudio Okay, so regardless of what operating system you’re using, the next thing we need to do is download and install RStudio. To understand why we install another program in addition to R, you need to understand a little bit more about R itself. The term R doesn’t really refer to a specific application on your computer. Rather, it refers to the underlying statistical language. You can use this language through lots of different applications. When you install R initially, it comes with one application that lets you do this: it’s the R.exe application on a Windows machine, and the R.app application on a Mac. But that’s not the only way to do it. There are lots of different applications that you can use that will let you interact with R. One of those is called RStudio, and it’s the one I’m going to suggest that you use. RStudio provides a clean, professional interface to R that I find much nicer to work with than either the Windows or Mac defaults. Like R itself, RStudio is free software: you can find all the details on their website. Download RStudio here: http://www.rstudio.com When you visit the RStudio website, you’ll probably be struck by how much cleaner and simpler it is than the CRAN website, and how obvious it is what you need to do: click the button that says “Download”3. When you click on the download button on the homepage it will ask you to choose whether you want the desktop version or the server version. You want the desktop version. After choosing the desktop version it will take you to a page (http://www.rstudio.org/download/desktop) that shows several possible downloads: there’s a different one for each operating system. However, the nice people at RStudio have designed the webpage so that it automatically recommends the download that is most appropriate for your computer. Click on the appropriate link, and the RStudio installer file will start downloading. Once it’s finished downloading, open the installer file in the usual way to install RStudio. After it’s finished installing, you can start R by opening RStudio. You don’t need to open R.app or R.exe in order to access R. RStudio will take care of that for you. To illustrate what RStudio looks like, Figure 0.1 shows a screenshot of an R session in progress. In this screenshot, you can see that it’s running on a Mac, but it looks almost identical no matter what operating system you have. The Windows version looks more like a Windows application (e.g., the menus are attached to the application window and the colour scheme is slightly different), but it’s more or less identical. There are a few minor differences in where things are located in the menus (I’ll point them out as we go along) and in the shortcut keys, because RStudio is trying to “feel” like a proper Mac application or a proper Windows application, and this means that it has to change its behaviour a little bit depending on what computer it’s running on. Even so, these differences are very small. One final note: the website RStudio Cloud even allows you to run R scripts in the cloud, so you can also practice R from your web-browser, although this has some limitations (most importantly: the amount of hours you can use R in the cloud is limited for free accounts). It is therefore highly recommended to do your course work on your personal device, but RStudio Cloud can help you out in a pinch. 0.4 Understanding RStudio Figure 0.1: The RStudio workspace 0.4.1 Console When you open up RStudio you will see three or four main windows (the placement of each are configurable). In the above example, the bottom left window is the command line (terminal or console) for R. Let’s focus on the console for now. The console is used to directly enter commands into R. There probably is a whole lot of text that doesn’t make much sense. It should look something like this: R version 4.0.2 (2020-06-22) -- &quot;Taking Off Again&quot; Copyright (C) 2020 The R Foundation for Statistical Computing Platform: x86_64-apple-darwin17.0 (64-bit) R is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type &#39;license()&#39; or &#39;licence()&#39; for distribution details. Natural language support but running in an English locale R is a collaborative project with many contributors. Type &#39;contributors()&#39; for more information and &#39;citation()&#39; on how to cite R or R packages in publications. Type &#39;demo()&#39; for some demos, &#39;help()&#39; for on-line help, or &#39;help.start()&#39; for an HTML browser interface to help. Type &#39;q()&#39; to quit R. &gt; Most of this text is pretty uninteresting, and when doing real data analysis you’ll never really pay much attention to it. The important part of it is this… &gt; … which has a flashing cursor next to it. That’s the command prompt. When you see this, it means that R is waiting patiently for you to do something! One of the easiest things you can do with R is use it as a simple calculator, so it’s a good place to start. For instance, try typing 10 + 20, and hitting enter.4 When you do this, you’ve entered a command, and R will “execute” that command. What you see on screen now will be this: &gt; 10 + 20 [1] 30 Not a lot of surprises in this extract. But there’s a few things worth talking about, even with such a simple example. Firstly, it’s important that you understand how to read the extract. In this example, what I typed was the 10 + 20 part. I didn’t type the &gt; symbol: that’s just the R command prompt and isn’t part of the actual command. And neither did I type the [1] 30 part. That’s what R printed out in response to my command. Secondly, it’s important to understand how the output is formatted. Obviously, the correct answer to the sum 10 + 20 is 30, and not surprisingly R has printed that out as part of its response. But it’s also printed out this [1] part, which probably doesn’t make a lot of sense to you right now. You’re going to see that a lot. I’ll talk about what this means in a bit more detail later on, but for now you can think of [1] 30 as if R were saying “the answer to the 1st question you asked is 30.” The console is useful for entering single lines of code and running them. Often times this occurs when you are learning how to correctly execute a line of code in R. Your first few attempts may be incorrect resulting in errors, but trying out different variations on your code in the command line can help you produce the correct code. Pressing the up arrow while in the console will scroll through the most recently executed lines of code. 0.4.2 Script Editor The top left corner contains the script editor. This is a simple text editor for writing and saving R scripts with many lines. Several tabs can be opened at once, with each tab representing a different R script. R scripts can be saved from the editor (resulting in a .R file). Whole scripts can be run by copy and pasting them into the console and pressing enter or “sourcing” them with the source button (light blue arrow). Sourcing will run all the code in your script without the need to copy/paste. Alternatively, you can highlight portions of the script that you want to run (in the script editor) and pressing the button for running the current line/section: green arrow pointing right. 0.4.3 Workspace and History The top right panel contains (at least) two tabs, one for the workspace and another for history. The workspace lists out all of the variables and functions that are currently loaded in R’s memory. You can inspect each of the variables by clicking on them. This is generally only useful for variables that do not contain large amounts of information. The history tab provides a record of the recent commands executed in the console. 0.4.4 File, Plot, Packages, Help The bottom-right window has four tabs for files, plots, packages, and help. The files tab allows browsing of the computers file directory. An important concept in R is the current working directory. This is file folder that R points to by default. Many functions in R will save things directly to this directory, or attempt to read files from this directory. The current working directory can be changed by navigating to the desired folder in the file menu, and then clicking on the more option to set that folder to the current working directory. This is especially important when loading data into R. The current working directory should be set to the folder containing the data to be inputted into R. The plots tab will show recent plots and figures made in R. The packages tab lists the current R libraries loaded into memory, and provides the ability to download and enable new R packages (more about that later). The help menu is an invaluable tool. Here, you can search for individual R commands to see examples of how they are used. Sometimes the help files for individual commands are opaque and difficult to understand, so it is necessary to do a Google search to find better examples of using these commands. 0.4.5 Installing libraries When you install R and RStudio, you get what is called Base R. Base R contains many libraries that allow you to conduct statistical analyses. Because R is free and open-source, many other developers have created add-on libraries that extend the functionality of R. We use some of these libraries, and you need to install them before you can do the labs. For example, in any of the labs, whenever you see a line code that uses the word library like this library(libraryname), this line of code telling R to load up that library so it can be used. The libraryname would be replaced with the actual name of the library. For example, you will see code like this in the labs: library(tidyverse) This line of code is saying that the tidyverse library needs to be loaded (tidyverse is an all purpose library which allows you to import data, clean up data and generate beautiful graphs and figures). However, before a library can be loaded, it needs to be downloaded and installed first. Fortunately, we can tell R to install all of the packages we need for this course in one go. Copy the following lines of code into the console, and press enter. install.packages(&quot;tidyverse&quot;) install.packages(&quot;gapminder&quot;) install.packages(&quot;ggpubr&quot;) install.packages(&quot;knitr&quot;) install.packages(&quot;rmarkdown&quot;) install.packages(&quot;bookdown&quot;) Note you can select all of the lines at once, then copy them, then paste all of them into the console, and press enter to run them all. After each of the packages are installed, you will then be able to load them using library(). You can see the installed packages by pressing the packages tab in the bottom right window. From this tab you can also update and install packages by clicking the update or install button respectively. 0.5 How to complete the labs Each of the labs focuses on particular data-analysis problems, from graphing data, computing descriptive statistics, to running inferential tests. Additionally, we will discuss important research design concepts. 0.5.1 R project To answer the questions during the labs, we will work in RStudio using an R project. An R project is a very useful way of organizing your files all in one place so you can find them later. When you double-click an R project file to open it, RStudio automatically loads and restores your last session. We have made an R project specifically for the labs that can be downloaded here or on Canvas. Download the .zip file and unzip (unarchive) it in a folder you can easily access. In the folder you unzip the file you should find the following: A folder titled “Labs_Template” Inside the folder you will see the “Labs.proj” project file and the template for the first lab (Lab01_Introduction.Rmd) A data folder containing data files for the labs By double clicking and opening the R project file, you make sure you have everything set up and are working in the correct directory for the labs. 0.5.2 RMarkdown During the labs, you will write your notes and answers in a .Rmd (RMarkdown) file. The RMarkdown template file for the first lab is already included and can be found in your Labs_Template folder (Lab01_Introduction.Rmd). We admit that at the beginning, RMarkdown documents might seem a little bit confusing, but you will find they are extremely useful and flexible. Basically, what RMarkdown allows you to do is combine two kinds of writing, 1) writing normal text, with headers, sub-headers, and paragraphs and 2) R code to conduct analyses. You can think of this like a lab journal, that contains both your writing about what you are doing, and the code that you used for the analyses. Additionally, when your code does something like make a graph, or run a statistical test, you can ask RMarkdown to print the results in your document.5 You complete each lab by documenting your progression through each of the parts of the lab in the RMarkdown template. At the end of each lab you will “knit” the RMarkdown file to a Word document (knitting is a term for making your documents look prettier) and upload the result to Canvas. By doing this, you will become familiar with how R and RStudio works, and how to create documents that preserve both the code and your notes all in one place. This getting started guide was adapted from the lab manual by Matthew Crump and Learning statistics with R by Danielle Navarro.↩︎ Other well-known statistical software packages are SPSS, SAS, JASP, Julia, MATLAB, Python with SciPy packages and the list goes on.↩︎ This is probably no coincidence: the people who design and distribute the core R language itself are focused on technical stuff. And sometimes they almost seem to forget that there’s an actual human user at the end. The people who design and distribute RStudio are focused on user interface. They want to make R as usable as possible. The two websites reflect that difference.↩︎ Seriously. If you’re in a position to do so, open up R and start typing. The simple act of typing it rather than “just reading” makes a big difference. It makes the concepts more concrete, and it ties the abstract ideas (programming and statistics) to the actual context in which you need to use them. Statistics is something you do, not just something you read about in a textbook.↩︎ The RMarkdown website has an excellent tutorial that is well worth checking out in your own time: https://rmarkdown.rstudio.com/lesson-1.html↩︎ "],["week-1-introductionweek1-introduction-1.html", " 1 Week 1: Introduction6 1.1 Learning goals 1.2 How to use this lab manual 1.3 RMarkdown basics 1.4 Research methods: measurement 1.5 Basic R", " 1 Week 1: Introduction6 This first lab consists of several distinct parts. In the first part we will take some time to get acquainted with RMarkdown files. In the next part will work on several exercises related to the readings about research methods. In the final part you be introduced to programming concepts in R. This lab assumes you have read the required readings of Week 1 and completed the Getting started guide. You will complete each lab by writing your notes and R code in an RMarkdown document and uploading the results to Canvas. You should have setup your files as described in this part of the Getting started section. When we made this course, we assumed that most students would be unfamiliar with R, and might even be frightened of it. Don’t worry. It’s going to be easier than you think. We know that it will seem challenging at first. But, we think that with lots of working examples, you will get the hang of it, and by the end of the course you will be able to do things you might never have dreamed you can do. It’s really a fantastic skill to learn, even if you aren’t planning on going on to do research. Before getting started we want to set a couple of expectations: Expect things to break. This is the nature of using computers to do stuff. When things break, it is easy to get frustrated. It is not always immediately obvious why something is not working (even for tutors), but in the end, having things break will help you better understand the materials. Self direct your learning. Do you understand all materials of a week? Set yourself a programming challenge that goes further than the intended learning goals. Do you find a particular concept difficult? Ask someone else to help you. Use one of the additional learning resources on Canvas, or Google for a website/video explaining the concept. The only way to learn is by doing, for this course even more so than other courses. Please follow the materials of the labs and try not to skip any exercises or code examples. If you do skip parts of a lab, you might quickly find yourself unable to understand more advanced concepts later on. You will soon learn that the depth of this course is essentially limitless and there is almost always more than one way to answer something, or even a “correct answer” at all. This is part of the fun! It is impossible to remember everything R can do, so one of the most important skills you can develop is knowing how and where to search for things you do not know. Having set your expectations, let’s get started with the first lab! 1.1 Learning goals During this lab you will do the following: Learn how to use this lab manual and the lab template Learn RMarkdown basics and how to knit an RMarkdown document Discuss fundamental concepts of research methods and design Take your first steps in R and RStudio Learn about operators, functions, variables and comments in R Learn about getting help in R and debugging common errors 1.2 How to use this lab manual The lab manual is your reference guide for completing the lab exercises. The template you download for a lab contains the exercises and will guide you through the materials of each lab. In the template we will reference to particular parts of the lab manual you should read and parts of the lab manual with additional materials. These additional materials help you extend your understanding of the materials and can be very useful working on your assignments during this course, or while conducting quantitative analyses in different courses. Follow the steps below to open R Studio and your template for this lab: Double-click the “Labs.Rproj” file in the Labs_Template directory (i.e., the place where you downloaded and unzipped the Labs_Template.zip file) RStudio should now start You should see some files and a data folder inside the “Labs_Template” folder (bottom right pane) Click the lab template file (Lab01_Introduction.Rmd) and it will load into the editor window You should keep your notes, copy/paste R code, and answer the questions of this lab in the lab template. Once you have opened the template your RStudio should look something like this: The upper left window pane is the place where the template has opened. Switch over to that window now and start reading the template. 1.3 RMarkdown basics As we mentioned in the Getting started guide, RMarkdown allows you to combine two kinds of writing: writing normal text, with headers, sub-headers, and paragraphs R code to conduct analyses This makes RMarkdown documents really useful for conducting quantitative research. You get the keep your analyses and written text in one place, and you can easily share your work with collaborators that can reproduce the analyses you have conducted. RMarkdown documents are also really versatile. In fact, both the textbook, as well as this lab manual, were written in RMarkdown. If you are used to working with a word processor like Microsoft Word, you might need a few minutes to get used to writing documents in RMarkdown, but you will quickly pick this up. The key difference is that RMarkdown keeps two things separate: your text and the formatting of your text, so you do not directly see how your text will look like in the final document. Let’s see what I mean by looking at an example. If I want to embolden a word, I write the following in my RMarkdown document: This is a **bold** word. Which will be displayed in my final document as: This is a bold word. A similar thing goes for italicizing a word: *This is displayed in italics* Which will display as: This is displayed in italics The list of things you can do to format your text in RMarkdown is extensive. You can find a cheatsheet with all the things you can do here or download the cheatsheet from Canvas. To include R code in your RMarkdown document, which we are going to do all the time during the labs and assignments, you should use an R code chunk. You can do that by inserting three backticks (```) followed by {r} (to indicate you are going to run R code). You finish a code chunk with another three backticks (```). In your RMarkdown document this will look as follows: Which will display as: # My first code chunk! 50 + 25 ## [1] 75 Make sure you start and close each code chunk with the three backticks (```). If you don’t do that, RStudio doesn’t know where your code starts and ends and will get utterly confused and start shouting errors and warnings at you. This is one of those mistakes that is really easy to make, but can take you forever to figure out, especially when you are just beginning with RMarkdown. By the way, do you see the green play button in the code chunk? If you press that, RStudio will execute the R code in your code chunk and display the result. You will likely use this button all the time during the labs, to check if your solutions to the exercises are correct. There’s one final concept about RMarkdown we need to introduce before you can get started with the remainder of this lab. This has to do with “knitting documents.” Knitting is the process of taking your text and RMarkdown markup and merge everything together to output it to a file. Knitting is done by pressing the knit button: Knitting creates a formatted document which can be displayed by other programs. By default, the document is knitted as a .html file (which can be read by any web browser), but you can also knit your RMarkdown documents as a Word document (.docx). To do so, press the downward arrow next to the knitting symbol and select “Knit to Word.” For your lab submission, you should submit a .docx file to Canvas. You should now be ready to complete the RMarkdown exercises in your template file, so switch over to RStudio! 1.3.0.1 RMarkdown exercises What does the # symbol do in RMarkdown? What about two ##? Insert an R code chunk to calculate the result of 10 + 20. Knit your document to a Word file and have a look at the result. 1.4 Research methods: measurement Discuss the research methods questions about measurement in your breakout room and register the answers in the R Markdown template. 1.4.1 Question 1 In the required readings of this week we called to process of clarifying abstract concepts and translating them into specific, observable measures operationalization. Operationalization involves both a nominal and an operational definition. Describe in your own words what these terms mean. 1.4.2 Question 2 Two different definitions of emotional well-being are provided by the Mental Health Foundation. For each of the following definitions, decide whether it constitutes a nominal or an operational definition: “A positive sense of well-being which enables an individual to be able to function in society and meet the demands of everyday life.” “People in good mental health have the ability to recover effectively from illness, change or misfortune.” 1.4.3 Question 3 Two different definitions of financial literacy can be found in literature. For each of the following definitions, decide whether it constitutes a nominal or an operational definition: “The ability to read, analyze, manage and communicate about the personal financial conditions that affect material well-being.” “The ability to manage effectively personal savings, credits and borrowed money as well as personal investments.” 1.4.4 Question 4 Suppose you want to study financial literacy, given the numerous benefits it brings to society, and given the documented lack of financial education. Would you use the following operational definition of financial literacy: “The ability to correctly predict short term fluctuations in the stock market?” 1.4.5 Question 5 The graph below is a visual representation of the concepts of measure validity and reliability. Figure 1.1: A visual representation of the concepts measure validity and reliability. Imagine the theoretical construct you want to measure is the bullseye of the dartboard and the dots represent an attempt at measurement. Illustration adapted from an illustration by Nevit Dilmen, Wikimedia Commons. For each one of the three statements below, indicate whether it corresponds to dart board A, B, C or none. The measure of our concept is valid, but not reliable. The measure of our concept is reliable, but not valid. The measure of our concept is neither valid, not reliable. The measure of our concept is both valid and reliable. 1.4.6 Question 6 The National Health Care Institute of the Netherlands partners with local schools to provide a weekly physical exercise program for children ages 6-14. The sessions are designed to last throughout the whole academic year, and they will take place in afternoon hours. They also consist of both a theoretical and a practical part. In the theoretical part, volunteers strive to increase children’s exercise habits by teaching them about the benefits of regular exercise, whereas in the practical part, they organize various age-appropriate sports activities for children to participate in. Changes in exercise habits are measured via a questionnaire at the end of the program. However, the program manager is concerned that the questionnaire is not producing high-quality observations, particularly for questions that ask children about their exercise habits before participating in the program. Assuming the problem is with measurement and not with the program design: What is the most likely measurement problem? Reliability or validity? What type of error is most likely producing this problem? Constant error, random error and/or correlated error? How might the program address this measurement problem? 1.4.7 Question 7 The Dutch Environmental Assessment Agency aims to identify sections of Dutch rivers for stream bank restoration. The goal of this work is to create stream bank conditions that can lead to eventual water quality improvements. Crews of national service volunteers implement remediation in accordance with the waterway management plan, including removal of trash and debris from stream banks, removal of invasive plants, reintroduction of native plants, and erosion abatement. Land managers from the Ministry of Infrastructure and Water Management inspect project sites within two weeks of project completion. The assessment instrument used by land managers contains checkbox items to indicate whether various remediation actions were taken but does not provide a way to assess the quality of these remediation actions with respect to environmental standards. This problem should be of high concern to the land managers, given the fact that high quality environmental standards are hard to meet, even when all the appropriate actions have been taken. Assuming the problem is with measurement and not with the program design: What is the most likely measurement problem? Reliability or validity? What type of error is most likely producing this problem? Constant error, random error and/or correlated error? How might the program address this measurement problem? 1.5 Basic R This part of the lab manual will introduce you to the very basics of R. You are urged to follow along with the examples in your own RStudio window. The answers to the exercises should be registered in the RMarkdown template. During this part of the lab, we’ll spend a bit of time using R as a simple calculator, since that’s the easiest thing to do with R, just to give you a feel for what it’s like to work in R. In the Getting started guide we learned to execute our first command in R, by typing 10 + 20 in the console and pressing enter. Try it out in the console of RStudio: 10+20 ## [1] 30 You can also type the command above in a code block in your template file and execute it there. That way, when you knit the template, the code examples are also included in your notes, which can be very helpful for working on your assignments or preparing for your exam. 1.5.1 Doing simple calculations with R First, let’s learn how to use one of the most powerful piece of statistical software in the world as a €2 calculator. So far, all we know how to do is addition. Clearly, a calculator that only did addition would be a bit stupid, so I should tell you about how to perform other simple calculations using R. But first, some more terminology. Addition is an example of an “operation” that you can perform (specifically, an arithmetic operation), and the operator that performs it is +. To people with a programming or mathematics background, this terminology probably feels pretty natural, but to other people it might feel like I’m trying to make something very simple (addition) sound more complicated than it is (by calling it an arithmetic operation). To some extent, that’s true: if addition was the only operation that we were interested in, it’d be a bit silly to introduce all this extra terminology. However, as we go along, we’ll start using more and more different kinds of operations, so it’s probably a good idea to get the language straight now, while we’re still talking about very familiar concepts like addition! 1.5.1.1 Adding, subtracting, multiplying and dividing So, now that we have the terminology, let’s learn how to perform some arithmetic operations in R. To that end, the table below lists the operators that correspond to the basic arithmetic we learned in primary school: addition, subtraction, multiplication and division. operation operator example input example output addition + 10 + 2 12 subtraction - 9 - 3 6 multiplication * 5 * 5 25 division / 10 / 3 3 power ^ 5 ^ 2 25 As you can see, R uses fairly standard symbols to denote each of the different operations you might want to perform: addition is done using the + operator, subtraction is performed by the - operator, and so on. So if I wanted to find out what 57 times 61 is (and who wouldn’t?), I can use R instead of a calculator, like so: 57 * 61 ## [1] 3477 So that’s handy. 1.5.1.2 Doing calculations in the right order Okay. At this point, you know how to take one of the most powerful pieces of statistical software in the world, and use it as a €2 calculator. And as a bonus, you’ve learned a few very basic programming concepts. That’s not nothing (you could argue that you’ve just saved yourself €2) but on the other hand, it’s not very much either. In order to use R more effectively, we need to introduce more programming concepts. In most situations where you would want to use a calculator, you might want to do multiple calculations. R lets you do this, just by typing in longer commands. 1 + 2 * 4 ## [1] 9 Clearly, this isn’t a problem for R either. However, it’s worth stopping for a second, and thinking about what R just did. Clearly, since it gave us an answer of 9 it must have multiplied 2 * 4 (to get an interim answer of 8) and then added 1 to that. But, suppose it had decided to just go from left to right: if R had decided instead to add 1+2 (to get an interim answer of 3) and then multiplied by 4, it would have come up with an answer of 12. To answer this, you need to know the order of operations that R uses. It’s actually the same order that (most of) you got taught when you were in high school: the “BEDMAS” order7. That is, first calculate things inside Brackets (), then calculate Exponents ^, then Division / and Multiplication *, then Addition + and Subtraction -. So, to continue the example above, if we want to force R to calculate the 1+2 part before the multiplication, all we would have to do is enclose it in brackets: (1 + 2) * 4 ## [1] 12 This is a fairly useful thing to be able to do. The only other thing I should point out about order of operations is what to expect when you have two operations that have the same priority: that is, how does R resolve ties? For instance, multiplication and division are actually the same priority, but what should we expect when we give R a problem like 4 / 2 * 3 to solve? If it evaluates the multiplication first and then the division, it would calculate a value of two-thirds. But if it evaluates the division first it calculates a value of 6. The answer, in this case, is that R goes from left to right, so in this case the division step would come first: 4 / 2 * 3 ## [1] 6 All of the above being said, it’s helpful to remember that brackets always come first. So, if you’re ever unsure about what order R will do things in, an easy solution is to enclose the thing you want it to do first in brackets. There’s nothing stopping you from typing (4 / 2) * 3. By enclosing the division in brackets we make it clear which thing is supposed to happen first. In this instance you wouldn’t have needed to, since R would have done the division first anyway, but when you’re first starting out it’s better to make sure R does what you want! 1.5.1.3 Arithmetics exercises Complete the following exercises in your lab template. Take your favorite number to the third power. Calculate the number of seconds in a year, on the simplifying assumption that a year contains exactly 365 days. Use R to calculate solution to 6/2*(1+2). Why is the solution not 1? 1.5.2 Using functions to do calculations The symbols +, -, * and so on are examples of operators. As we’ve seen, you can do quite a lot of calculations just by using these operators. However, in order to do more advanced calculations (and later on, to do actual statistics), you’re going to need to start using functions. To get started, suppose I wanted to take the square root of 225. The square root, in case your high school maths is a bit rusty, is just the opposite of squaring a number. So, for instance, since “5 squared is 25” I can say that “5 is the square root of 25.” The usual notation for this is \\[ \\sqrt{25} = 5 \\] though sometimes you’ll also see it written like this \\(25^{0.5} = 5.\\) To calculate the square root of 25, I can do it in my head pretty easily, since I memorised my multiplication tables when I was a kid. It gets harder when the numbers get bigger, and pretty much impossible if they’re not whole numbers. This is where something like R comes in very handy. Let’s say I wanted to calculate \\(\\sqrt{225}\\), the square root of 225. There’s two ways I could do this using R. Firstly, since the square root of 255 is the same thing as raising 225 to the power of 0.5, I could use the power operator ^, just like we did earlier: 225 ^ 0.5 ## [1] 15 However, there’s a second way that we can do this, since R also provides a square root function: sqrt(). To calculate the square root of 255 using this function, what I do is insert the number 225 in the parentheses. That is, the command I type is this: sqrt(225) ## [1] 15 When we use a function to do something, we generally refer to this as calling the function, and the values that we type into the function (there can be more than one) are referred to as the arguments of that function. Obviously, the sqrt() function doesn’t really give us any new functionality, since we already knew how to do square root calculations by using the power operator ^, though I do think it looks nicer when we use sqrt(). However, there are lots of other functions in R: in fact, almost everything of interest that we’ll use during our statistical analyses is an R function of some kind. For example, one function that can come in handy is the absolute value function. Compared to the square root function, it’s extremely simple: it just converts negative numbers to positive numbers, and leaves positive numbers alone. Calculating absolute values in R is pretty easy, since R provides the abs function that you can use for this purpose. For instance: abs(-13) ## [1] 13 Before moving on, it’s worth noting that – in the same way that R allows us to put multiple operations together into a longer command, like 1 + 2*4 for instance – it also lets us put functions together and even combine functions with operators if we so desire. For example, the following is a perfectly legitimate command: sqrt( 1 + abs(-8) ) ## [1] 3 When R executes this command, starts out by calculating the value of abs(-8), which produces an intermediate value of 8. Having done so, the command simplifies to sqrt( 1 + 8 ). 1.5.2.1 Multiple arguments There’s two more fairly important things that you need to understand about how functions work in R, and that’s the use of “named” arguments, and default values\" for arguments. Not surprisingly, that’s not to say that this is the last we’ll hear about how functions work, but they are the last things we desperately need to discuss in order to get you started. To understand what these two concepts are all about, I’ll introduce another function. The round() function can be used to round some value to the nearest whole number. For example, I could type this: round(3.1415) ## [1] 3 Pretty straightforward, really. However, suppose I only wanted to round it to two decimal places: that is, I want to get 3.14 as the output. The round() function supports this, by allowing you to input a second argument to the function that specifies the number of decimal places that you want to round the number to. In other words, I could do this: round(3.1415, 2) ## [1] 3.14 What’s happening here is that I’ve specified two arguments: the first argument is the number that needs to be rounded (i.e., 3.1415), the second argument is the number of decimal places that it should be rounded to (i.e., 2), and the two arguments are separated by a comma. 1.5.2.2 Argument names In this simple example, it’s quite easy to remember which one argument comes first and which one comes second, but for more complicated functions this is not easy. Fortunately, most R functions make use of argument names. For the round() function, for example the number that needs to be rounded is specified using the x argument, and the number of decimal points that you want it rounded to is specified using the digits argument. Because we have these names available to us, we can specify the arguments to the function by name. We do so like this: round(x = 3.1415, digits = 2) ## [1] 3.14 Notice that this is kind of similar in spirit to variable assignment, except that I used = here, rather than &lt;-. In both cases we’re specifying specific values to be associated with a label. However, there are some differences between what I was doing earlier on when creating variables, and what I’m doing here when specifying arguments, and so as a consequence it’s important that you use = in this context. As you can see, specifying the arguments by name involves a lot more typing, but it’s also a lot easier to read. Because of this, the commands in this lab manual will usually specify arguments by name, since that makes it clearer to you what I’m doing. However, one important thing to note is that when specifying the arguments using their names, it doesn’t matter what order you type them in. But if you don’t use the argument names, then you have to input the arguments in the correct order. In other words, these three commands all produce the same output… round(3.1415, 2) ## [1] 3.14 round(x = 3.1415, digits = 2) ## [1] 3.14 round(digits = 2, x = 3.1415) ## [1] 3.14 but this one does not… round( 2, 3.14165 ) ## [1] 2 1.5.2.3 Getting help with functions How do you find out what the correct order is or what arguments a function uses? There’s a few different ways, but the easiest one is to look at the help documentation for the function. You can look up the documentation of any function by typing a question mark (?) and the function name as follows: ?round I have somewhat mixed feelings about the help documentation in R. On the plus side, there’s a lot of it, and it’s very thorough. On the minus side, there’s a lot of it, and it’s very thorough. There’s so much help documentation that it sometimes doesn’t help, and most of it is written with an advanced user in mind. Now, it’s probably beginning to dawn on you that there are going to be a lot of R functions, all of which have their own arguments. You’re probably also worried that you’re going to have to remember all of them! Thankfully, it’s not that bad. In fact, very few data analysts bother to try to remember all the commands. What they really do is use tricks to make their lives easier. The first trick is using the ? command shown above to display the documentation on a particular function. Another trick is to use two question marks (??) to launch a search to all mentions of the word after ?? in the R documentation. The final, and arguably most important trick, is to use the internet. If you don’t know how a particular R function works, or you want to do something in R but are unsure how, Google it. 1.5.2.4 Function exercises Complete the following exercises in your lab template. Use a function to calculate the square root of your favorite number. How many arguments does the function log() take? Use R to execute the following command: rep(\"hello!\",100). What does the rep() function do? Could you rewrite the command to use argument names? 1.5.3 Storing a number as a variable One of the most important things to be able to do in R (or any programming language, for that matter) is to store information in variables. Variables in R aren’t exactly the same thing as the variables we talked about in the chapter on research methods, but they are similar. At a conceptual level you can think of a variable as label for a certain piece of information, or even several different pieces of information. When doing statistical analysis in R all of your data (the variables you measured in your study) will be stored as variables in R, but as well see later in the book you’ll find that you end up creating variables for other things too. However, before we delve into all the messy details of data sets and statistical analysis, let’s look at the very basics for how we create variables and work with them. 1.5.3.1 Variable assignment using &lt;- Since we’ve been working with numbers so far, let’s start by creating variables to store our numbers. And since most people like concrete examples, let’s invent one. Suppose I’m trying to calculate how much money I’m going to make from selling my book about statistics. There’s several different numbers I might want to store. Firstly, I need to figure out how many copies I’ll sell. The book I’m writing isn’t exactly Harry Potter, so let’s assume I’m only going to sell one copy per student in my class. That’s about 200 sales, so let’s create a variable called sales. What I want to do is assign a value to my variable sales, and that value should be 200. We do this by using the assignment operator, which is &lt;-. Here’s how we do it: sales &lt;- 200 When you hit enter, R doesn’t print out any output.8 It just gives you another command prompt. However, behind the scenes R has created a variable called sales and given it a value of 200. You can check that this has happened by asking R to print the variable on screen. And the simplest way to do that is to type the name of the variable and hit enter. sales ## [1] 200 1.5.3.2 Doing calculations using variables Okay, let’s get back to my original story. In my quest to become rich, I’ve written this statistics textbook. To figure out how good a strategy this is, I’ve started creating some variables in R. In addition to defining a sales variable that counts the number of copies I’m going to sell, I can also create a variable called royalty, indicating how much money I get per copy. Let’s say that my royalties are about €7 per book: sales &lt;- 200 royalty &lt;- 7 The nice thing about variables (in fact, the whole point of having variables) is that we can do anything with a variable that we ought to be able to do with the information that it stores. That is, since R allows me to multiply 200 by 7 200 * 7 ## [1] 1400 it also allows me to multiply sales by royalty sales * royalty ## [1] 1400 As far as R is concerned, the sales * royalty command is the same as the 200 * 7 command. Not surprisingly, I can assign the output of this calculation to a new variable, which I’ll call revenue. And when we do this, the new variable revenue gets the value 1400. So let’s do that, and then get R to print out the value of revenue so that we can verify that it’s done what we asked: revenue &lt;- sales * royalty revenue ## [1] 1400 That’s fairly straightforward. A slightly more subtle thing we can do is reassign the value of my variable, based on its current value. For instance, suppose that one of my students loves the book so much that he or she donates me an extra €550. The simplest way to capture this is by a command like this: revenue &lt;- revenue + 550 revenue ## [1] 1950 In this calculation, R has taken the old value of revenue (i.e., 1400) and added 550 to that value, producing a value of 1950. This new value is assigned to the revenue variable, overwriting its previous value. In any case, we now know that I’m expecting to make €1950 off this. Hurray! 1.5.3.3 Exercises variables Complete the following exercises in your lab template. Assign your favorite number to the variable fav_num. Assign a sequence of numbers from 1 to 10 the variable seq_10 (hint: seq()). Multiply fav_num with seq_10 and save the result in a variable called fav_num_seq10.9 1.5.4 Using comments Another very useful feature of R is the comment character, #. It has a simple meaning in R: it tells R to ignore everything else you’ve written on the line after the # character. You won’t have much need of the # character immediately, but it’s very when writing longer scripts. For instance, if you read this: seeker &lt;- 3.1415 # create the first variable lover &lt;- 2.7183 # create the second variable keeper &lt;- seeker * lover # now multiply them to create a third one print(keeper) # print out the value of &#39;keeper&#39; } it’s a lot easier to understand what I’m doing than if I just write this: seeker &lt;- 3.1415 lover &lt;- 2.7183 keeper &lt;- seeker * lover print(keeper) Commenting makes any code a little easier to understand. 1.5.5 R is pretty stupid? There are a couple of things you should keep in mind when working with R. The first thing is that, while R is good software, it’s still software. To some extent, I’m stating the obvious here, but it’s important. The people who wrote R are smart. You, the user, are smart. But R itself is dumb. And because it’s dumb, it has to be mindlessly obedient. It does exactly what you ask it to do. There is no equivalent to “autocorrect” in R, and for good reason. When doing advanced stuff – and even the simplest of statistics is pretty advanced in a lot of ways – it’s dangerous to let a mindless automaton like R try to overrule the human user. But because of this, it’s your responsibility to be careful. Always make sure you type exactly what you mean. When dealing with computers, it’s not enough to type “approximately” the right thing. In general, you absolutely must be precise in what you say to R … like all machines it is too stupid to be anything other than absurdly literal in its interpretation. 1.5.5.1 Typos R takes it on faith that you meant to type exactly what you did type. For example, suppose that you forgot to hit the shift key when trying to type +, and as a result your command ended up being 10 = 20 rather than 10 + 20. 10 = 20 What happens when you have R try to execute this command, is that it attempts to interpret 10 = 20 as a command, and spits out an error message because the command doesn’t make any sense. When a human looks at this, and then looks down at his or her keyboard and sees that + and = are on the same key, it’s pretty obvious that the command was a typo. But R doesn’t know this, so it gets upset. And, if you look at it from its perspective, this makes sense. All that R “knows” is that 10 is a legitimate number, 20 is a legitimate number, and = is a legitimate part of the language too. In other words, from its perspective this really does look like the user meant to type 10 = 20, since all the individual parts of that statement are legitimate and it’s too stupid to realise that this is probably a typo. Therefore, R takes it on faith that this is exactly what you meant… it only “discovers” that the command is nonsense when it tries to follow your instructions, typo and all. And then it whinges, and spits out an error. Even more subtle is the fact that some typos won’t produce errors at all, because they happen to correspond to “well-formed” R commands. For instance, suppose that not only did I forget to hit the shift key when trying to type 10 + 20, I also managed to press the key next to one I meant do. The resulting typo would produce the command 10 - 20. Clearly, R has no way of knowing that you meant to add 20 to 10, not subtract 20 from 10, so what happens this time is this: 10 - 20 ## [1] -10 In this case, R produces the right answer, but to the the wrong question. 1.5.5.2 R is flexible with spacing? I should point out that there are some exceptions. Or, more accurately, there are some situations in which R does show a bit more flexibility than my previous description suggests. The first thing R is smart enough to do is ignore redundant spacing. What I mean by this is that, when I typed 10 + 20 before, I could equally have done this 10 + 20 ## [1] 30 or this 10+20 ## [1] 30 and I would get exactly the same answer. However, that doesn’t mean that you can insert spaces in any old place. For example, the startup message of R suggests you can type citation() to get some information about how to cite R. If I do so… citation() ## ## To cite R in publications use: ## ## R Core Team (2020). R: A language and environment for statistical ## computing. R Foundation for Statistical Computing, Vienna, Austria. ## URL https://www.R-project.org/. ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {R: A Language and Environment for Statistical Computing}, ## author = {{R Core Team}}, ## organization = {R Foundation for Statistical Computing}, ## address = {Vienna, Austria}, ## year = {2020}, ## url = {https://www.R-project.org/}, ## } ## ## We have invested a lot of time and effort in creating R, please cite it ## when using it for data analysis. See also &#39;citation(&quot;pkgname&quot;)&#39; for ## citing R packages. … it tells me to cite the R manual (R Core Team 2020). Let’s see what happens when we try changing the spacing. If you insert spaces in between the word and the parentheses, or inside the parentheses themselves, then all is well. That is, either of these two commands citation () citation( ) will produce exactly the same response. However, what we can’t do is insert spaces in the middle of the word. If you try to do this, R gets upset: citat ion() Throughout this lab manual you will see varied uses of spacing, just to give you a feel for the different ways in which spacing can be used. We’ll try not to do it too much though, since it’s generally considered to be good practice to be consistent in how you format your commands. 1.5.5.3 R knows you’re not finished? One more thing we should point out. If you hit enter in a situation where it’s “obvious” to R that you haven’t actually finished typing the command, R is just smart enough to keep waiting. For example, if you type 10 + and then press enter, even R is smart enough to realise that you probably wanted to type in another number. So here’s what happens: &gt; 10+ + and there’s a blinking cursor next to the plus sign. What this means is that R is still waiting for you to finish. It “thinks” you’re still typing your command, so it hasn’t tried to execute it yet. In other words, this plus sign is actually another command prompt. It’s different from the usual one (i.e., the &gt; symbol) to remind you that R is going to “add” whatever you type now to what you typed last time. For example, if we then go on to type 20 and hit enter, what we get is this: &gt; 10 + + 20 [1] 30 And as far as R is concerned, this is exactly the same as if you had typed 10 + 20. Similarly, consider the citation() command that we talked about in the previous section. Suppose you hit enter after typing citation(. Once again, R is smart enough to realise that there must be more coming – since you need to add the ) character – so it waits. We can even hit enter several times and it will keep waiting: &gt; citation( + + + ) We’ll make use of this a lot in this book. A lot of the commands that we’ll have to type are pretty long, and they’re visually a bit easier to read if we break it up over several lines. If you start doing this yourself, you’ll eventually get yourself in trouble (it happens to us all). Maybe you start typing a command, and then you realise you’ve screwed up. For example, &gt; citblation( + + You’d probably prefer R not to try running this command, right? If you want to get out of this situation, just hit the ‘escape’ key.10 R will return you to the normal command prompt (i.e. &gt;) without attempting to execute the botched command. That being said, it’s not often the case that R is smart enough to tell that there’s more coming. For instance, in the same way that I can’t add a space in the middle of a word, I can’t hit enter in the middle of a word either. If we hit enter after typing citat we get an error, because R thinks we’re interested in an “object” called citat and can’t find it: &gt; citat Error: object &#39;citat&#39; not found What about if we typed citation and hit enter? In this case we get something very odd, something that we definitely don’t want, at least at this stage. Here’s what happens: citation ## function (package = &quot;base&quot;, lib.loc = NULL, auto = NULL) ## { ## dir &lt;- system.file(package = package, lib.loc = lib.loc) ## if (dir == &quot;&quot;) ## stop(gettextf(&quot;package &#39;%s&#39; not found&quot;, package), domain = NA) BLAH BLAH BLAH where the BLAH BLAH BLAH goes on for rather a long time, and you don’t know enough R yet to understand what all this gibberish actually means (of course, it doesn’t actually say BLAH BLAH BLAH - it says some other things we don’t understand or need to know that I’ve edited for length) This incomprehensible output can be quite intimidating to novice users, and unfortunately it’s very easy to forget to type the parentheses; so almost certainly you’ll do this by accident. Do not panic when this happens. Simply ignore the gibberish. As you become more experienced this gibberish will start to make sense, and you’ll find it quite handy to print this stuff out.11 But for now just try to remember to add the parentheses when typing your commands. 1.5.5.4 Common mistakes exercises Complete the following exercises in your lab template. Figure out what is wrong with the following R commands and try to fix them: Mistake 1 x &lt;- 1 y &lt;- 5 x*z Mistake 2 x &lt;- Seq(1,10) Mistake 3 x &lt;- sqrt(seq(1,10) Mistake 4 This is actually my favorite number: fav_num &lt;- 2.718 When you have completed all exercises and are happy with your progress today, please knit your document (as a .docx) and submit it to Canvas. If you are unable to finish the exercises during the lab, continue working on them at home and discuss the exercises with your peers. You should upload your document to Canvas by Monday 23:59. The exercises will not be graded, and you will not receive personal feedback on your answers, but they should show a good effort trying to complete the exercises. The answers to all exercises will be uploaded to Canvas every Monday night. If you still have questions after finishing the exercises and reviewing the answer key, please visit the office hours on Wednesday. If you finish before the time is up, you can start with the required readings of Week 2 or help out your fellow students. You can also have a look at the instructions for the first assignment and sign up for an assignment group. The R part of this lab was adapted from the book by Danielle Navarro↩︎ Alternatively: PEMDAS: Parentheses, Exponents, Multiplication, Division, Addition, Subtraction↩︎ If you are using RStudio, and the “environment” panel is visible when you typed the command, then you probably saw something happening there. That’s to be expected, and is quite helpful.↩︎ The output of this operation should result in a so-called vector of 10 numbers. We will encounter vectors later in the course, but basically a vector is a variable that can store multiple values.↩︎ If you’re running R from the terminal rather than from RStudio, escape doesn’t work: use CTRL-C instead.↩︎ For advanced users: yes, as you’ve probably guessed, R is printing out the source code for the function.↩︎ "],["week-2-describing-data.html", " 2 Week 2: Describing Data 2.1 Learning goals 2.2 Loading data 2.3 Making graphs 2.4 Gapminder dataset 2.5 Using numbers to describe data", " 2 Week 2: Describing Data The commonality between science and art is in trying to see profoundly - to develop strategies of seeing and showing. —Edward Tufte The purpose of this lab is to show you how to generate graphs and compute basic descriptive statistics, including measures of central tendency (mean, mode, median) and variation (range, variance, standard deviation). We learned in the lecture and from the textbook that data we want to answer questions of, often comes with loads of numbers. Too many numbers to look at all at once. That’s one reason we use descriptive statistics. To reduce the big set of numbers to one or two summary numbers or a pretty graph that tells us something about all of the numbers. R can produce descriptive statistics for you in many ways. We’ll go over some R basics for descriptive statistics, and then use our new found skills to ask some questions about real data. To get started, download the lab template here (right click: save as) or from Canvas. Copy the lab template to your lab folder, double-click Labs.proj to start RStudio and open the lab template. 2.1 Learning goals During this lab you will do the following: Learn how to load data into R Learn how data is structured Generate graphs using ggplot2 Compute measures of central tendency and variation using R Answer some questions about data using descriptive statistics 2.2 Loading data In order to load data, we need to have some data first… It turns out that NYC makes a lot of data about a lot things open and free for anyone to download and look at. I searched through the data, and found a data file that lists the locations of film permits for shooting movies all throughout the different boroughs of NYC. There are multiple ways to load this data into R and we are going to look at one way below. If you have downloaded the Labs_Template.zip file, then you should have the data file in the data folder of your Labs project. Assuming you are working in the lab directory, use the following commands to load the data into RStudio: library(tidyverse) nyc_films &lt;-read_csv(&quot;data/Film_Permits.csv&quot;) Make sure to follow along with the code examples and execute the code above in your RStudio console or (even better) in a code chunk in your lab template file! Try to understand what is happening in the code above. We first load something called a library in R. R can do a lot of things out of the box, but over time people have written functions that extend the base capabilities of R. The library we load is called tidyverse. Then, we use the function read_csv() (from the tidyverse library) to load the .csv file and assign it to the variable nyc_films. If you are having issues getting the data loaded, talk to your tutor. 2.2.1 First look at the data You will be downloading and analyzing all kinds of data files this quad. After loading a file, it is always a good idea to have a first look at your data and see what you’ve got. In the top right-hand corner of the RStudio window, in the environment tab, you should now see a variable called nyc_films. If you click it, it will show you the contents of the data in a new window. The data is stored in something we call a data.frame12. It’s R lingo, for the thing that contains the data. Notice it is shaped like a rectangle, with rows going across, and columns going up and down. It looks kind of like an excel spreadsheet if you are familiar with Excel. You can also use the R functions head() and tail() to print out the first 6 or last 6 rows of our data frame in the R console, or glimpse() for a quick overview of our data frame (again, follow along in your own RStudio): # head() prints the first 6 rows of a data frame head(nyc_films) # tail() prints the last 6 rows of a data frame tail(nyc_films) # glimpse() to show a brief summary of a data frame glimpse(nyc_films) Using the head(), tail() and glimpse() functions on this data.frame will give you a quick indication of its contents. You’ll see that this data frame is really big, it has 50,728 rows (or observations) and 14 columns (or variables). We can also use the $ operator to request the data of a single column (i.e. variable) from the data.frame. For example, to print the data of the first 6 rows of the Category column: head(nyc_films$Category) ## [1] &quot;Television&quot; &quot;Television&quot; &quot;Television&quot; &quot;Commercial&quot; &quot;Television&quot; ## [6] &quot;Television&quot; 2.2.1.1 Data exercises Complete the following exercises in your lab template. What are the last 6 entries of the Category column in nyc_films? What are the last 10 entries of the EventType column in nyc_films? (hint: use tail() with the n= argument) You can also load data directly from the internet using read_csv(). Use read_csv() to load the data from https://github.com/thomashulst/quantrma_lab/raw/master/data/Film_Permits.csv and assign it to the variable nyc_films2. 2.3 Making graphs 2.3.1 Wrangle first Let’s walk through a couple questions we might have about the NYC films data and make several graphs to visualize the answers to these questions. We saw there were 50,728 film permits made. We also saw that there were different columns telling us information about each of the film permits. For example, the Borough column lists the borough for each request, i.e. whether the permit was for: Manhattan, Brooklyn, Bronx, Queen’s, or Staten Island. We could ask: where are the most film permits being requested (i.e. what borough)? We can find out by using R to plot the data in a bar plot. A bar plot looks a bit like a histogram, but the bars on a bar plot represent the counts of categorical data instead of continuous data. Before we can make this plot, we need to have R count how many film permits are made in each borough (we surely don’t want to do that by hand!). This step is called wrangling (i.e. preparing your data for visualization and/or further analysis), which you might remember from the lecture. To prepare the nyc_films data for plotting, run the following code: counts &lt;- nyc_films %&gt;% group_by(Borough) %&gt;% summarize(count_of_permits = length(Borough)) What is happening in the code above? Step through the code line by line to see if you understand it. On the first line, we are using a new operator you haven’t encountered before: %&gt;%. This operator is called the ‘pipe’ operator. The pipe operator is part of the tidyverse library we loaded before. The pipe operator allows us to clearly express a sequence of multiple operations on a data frame. On the first line of the code above, we tell R: use the data frame nyc_films for the next operations and assign the result to the variable counts. In the second line of the code we group the data by each of the five boroughs. On the final line, we summarize the number of times each borough occurs using the length() function.13 We end up with a variable counts with two columns: the name of the boroughs (counts$Borough) and the number of permits counted (counts$count_of_permits). The code above is equivalent to the code below which doesn’t use the pipe operator, but assigns the outcome of an operation to an intermediate variable. nyc_films_grouped &lt;- group_by(nyc_films, Borough) counts &lt;- summarize(nyc_films_grouped, count_of_permits = length(Borough)) The difference between the first or second code block might seem a bit confusing or arbitrary, as both methods generate the same result, but when performing many operations on a data frame, the pipe operator results in much more efficient and readable code. Either way, if you click on the counts variable in your environment, you will see the five boroughs listed, along with the counts for how many film permits were requested in each Borough. Those are the numbers we want to plot in a graph. 2.3.2 Plotting counts We actually make the plot using ggplot(). ggplot is also part of the tidyverse and is very powerful once you get the hand of it, but it can be a bit intimidating in the beginning14. Like previous code examples, it helps if we demonstrate the code first and you follow along by copy/pasting and executing the code. You can then go very quickly to answering your own questions about data, by making minor adjustments to the code examples. Here’s the code to make the plot: ggplot(counts, aes(x = Borough, y = count_of_permits )) + geom_col() There it is, we’re done here! We can easily look at this graph, and answer our question. Most of the film permits were requested in Manhattan, followed by Brooklyn, then Queen’s, the Bronx, and finally Staten Island. Again, you might be skeptical of what you are doing here, just copying and pasting things. Soon you’ll see how fast you can answer questions about data by copying and pasting code and just making a few little changes. Let’s quickly answer another question about what kinds of films are being made to show you what we mean. The column Category contains information about the category of the permits. Let’s copy/paste the code we already used, and see count the categories the films fall into. See if you can tell what I changed in the code to make this work: counts &lt;- nyc_films %&gt;% group_by(Category) %&gt;% summarize(count_of_permits = length(Category)) ggplot(counts, aes(x = Category, y = count_of_permits )) + geom_col() OK, so this figure might look a bit weird because the labels on the bottom are running into each other. We’ll fix that in a bit. First, let’s notice the changes. I changed Borough to Category. That was the main thing. I left out a bunch of things from before. I didn’t need to re-run the very early code to get the data in the nyc_films variable. R already has those things in it’s memory, so we don’t need to do that again. So how do we fix the graph? Good question. You probably have no idea how to do this, but Googling your questions is a great way to learn R. It’s what everybody does. I Googled “rotating labels ggplot” and found lots of ways to fix the graph. The trick I used is to add the last line in the R code below. I just copy-pasted it from the solution I found on stack overflow (you will become friends with stack overflow this course, there are many solutions there to all of your questions). counts &lt;- nyc_films %&gt;% group_by(Category) %&gt;% summarize(count_of_permits = length(Category)) ggplot(counts, aes(x = Category, y = count_of_permits )) + geom_col()+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) Googling this is not the only way to answer your questions. In addition to the help function built into R (?), which to be honest can be quite confusing, you can also use R cheatsheets or look up the reference manual on the ggplot website. It would impossible to remember all of this from the top of your head! We cannot stress enough, part of learning to program, and doing quantitative data analysis, is knowing how and where to search for things you do not know. Before we go further, I want to point out some basic properties of ggplot, just to give you a sense of how it is working. The ggplot() function makes use of so-called layers. What are these layers? Well, it draws things from the bottom up. It lays down one layer of graphics, then you can keep adding on top, drawing more things. So the idea is something like: Layer 1 + Layer 2 + Layer 3, and so on. If you want Layer 3 to be Layer 2, then you just switch them in the code. Here is a way of thinking about ggplot code: ggplot(name_of_data, aes(x = name_of_x_variable, y = name_of_y_variable)) + geom_layer()+ geom_layer()+ geom_layer() What I want you to focus on in the above description is the \\(+\\) signs. What we are doing with the plus signs is adding layers to plot. The layers get added in the order that they are written (a bit like the %&gt;% operator for data frames). If you look back to our previous code, you will see we add a geom_col layer, then we added another layer to change the rotation of the words on the x-axis. This is how it works. Section 2.3.3 below contains additional materials which are very helpful when working on your assignments, but are not mandatory for completing this lab. Section 2.3.3 will explain how to add a title to your graph. Or change the labels on the axes. Or add different colors, or change the font-size, or change the background. You can change all of these things by adding different lines to the existing code demonstrated below. We will skip those materials for now. For this lab, after completing the Plot exercises below in your lab template, skip the additional materials in Section 2.3.3 and go directly to Section 2.4. 2.3.2.1 Plot exercises Complete the following exercises in your lab template: Explain in your own words what is happening in the code block below. What does the %&gt;% operator indicate? counts &lt;- nyc_films %&gt;% group_by(Category) %&gt;% summarize(count_of_permits = length(Category)) The first argument of ggplot() should always be the data argument. In what type of form should the data be provided? (hint: ?ggplot) What function can we use to set the title of a plot? Use the ggplot reference manual, the additional materials, or Google to answer this question. 2.3.3 Additional materials: ggplot() layers and functions These are additional materials for when you are working on your assignment and want to change to look of your graph. You can also return to these materials when doing quantitative analyses in different courses. 2.3.3.1 ylab() changes y label The last graph had count_of_permits as the label on the y-axis. That doesn’t look right. ggplot automatically took the label from the column name, and made it be the name on the y-axis. We can change that by adding ylab(\"text\"). We do this by adding a \\(+\\) to the last line, then adding ylab() ggplot(counts, aes(x = Category, y = count_of_permits )) + geom_col() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ylab(&quot;Number of Film Permits&quot;) 2.3.3.2 xlab() changes x label Let’s slightly modify the x label too: ggplot(counts, aes(x = Category, y = count_of_permits )) + geom_col() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ylab(&quot;Number of Film Permits&quot;) + xlab(&quot;Category of film&quot;) 2.3.3.3 ggtitle() adds title Let’s give our graph a title: ggplot(counts, aes(x = Category, y = count_of_permits )) + geom_col() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ylab(&quot;Number of Film Permits&quot;) + xlab(&quot;Category of film&quot;) + ggtitle(&quot;Number of Film permits in NYC by Category&quot;) 2.3.3.4 color() adds color Let’s make the bars different colors. To do this, we add new code to the inside of the aes() part: ggplot(counts, aes(x = Category, y = count_of_permits, color=Category )) + geom_col() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ylab(&quot;Number of Film Permits&quot;) + xlab(&quot;Category of film&quot;) + ggtitle(&quot;Number of Film permits in NYC by Category&quot;) 2.3.3.5 fill() fills in color Let’s make the bars different colors. To do this, we add new code to the inside of the aes() part…Notice I’ve started using new lines to make the code more readable. ggplot(counts, aes(x = Category, y = count_of_permits, color=Category, fill= Category )) + geom_col() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ylab(&quot;Number of Film Permits&quot;) + xlab(&quot;Category of film&quot;) + ggtitle(&quot;Number of Film permits in NYC by Category&quot;) 2.3.3.6 Get rid of the legend Sometimes you just don’t want the legend on the side, to remove it add theme(legend.position=\"none\") ggplot(counts, aes(x = Category, y = count_of_permits, color=Category, fill= Category )) + geom_col() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ylab(&quot;Number of Film Permits&quot;) + xlab(&quot;Category of film&quot;) + ggtitle(&quot;Number of Film permits in NYC by Category&quot;) + theme(legend.position=&quot;none&quot;) 2.3.3.7 theme_classic() makes white background The rest is often just visual preference. For example, the graph above has this grey grid behind the bars. For a clean classic no nonsense look, use theme_classic() to take away the grid. ggplot(counts, aes(x = Category, y = count_of_permits, color=Category, fill= Category )) + geom_col() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ylab(&quot;Number of Film Permits&quot;) + xlab(&quot;Category of film&quot;) + ggtitle(&quot;Number of Film permits in NYC by Category&quot;) + theme(legend.position=&quot;none&quot;) + theme_classic() 2.3.3.8 Sometimes layer order matters Interesting, theme_classic() is misbehaving a little bit and incorrectly renders the axis labels and reintroduces the legend. It looks like we have some of our layers out of order, let’s re-order. I just moved theme_classic() to just underneath the geom_bar() line. Now everything get’s drawn properly. ggplot(counts, aes(x = Category, y = count_of_permits, color=Category, fill= Category )) + geom_col() + theme_classic() + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ylab(&quot;Number of Film Permits&quot;) + xlab(&quot;Category of film&quot;) + ggtitle(&quot;Number of Film permits in NYC by Category&quot;) + theme(legend.position=&quot;none&quot;) 2.3.3.9 Font-size Changing font-size is often something you want to do. ggplot2 can do this in different ways. I suggest using the base_size option inside theme_classic(). You set one number for the largest font size in the graph, and everything else gets scaled to fit with that that first number. It’s really convenient. Look for the inside of theme_classic() ggplot(counts, aes(x = Category, y = count_of_permits, color=Category, fill= Category )) + geom_col() + theme_classic(base_size = 15) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ylab(&quot;Number of Film Permits&quot;) + xlab(&quot;Category of film&quot;) + ggtitle(&quot;Number of Film permits in NYC by Category&quot;) + theme(legend.position=&quot;none&quot;) or make things small… just to see what happens ggplot(counts, aes(x = Category, y = count_of_permits, color=Category, fill= Category )) + geom_col() + theme_classic(base_size = 10) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ylab(&quot;Number of Film Permits&quot;) + xlab(&quot;Category of film&quot;) + ggtitle(&quot;Number of Film permits in NYC by Category&quot;) + theme(legend.position=&quot;none&quot;) 2.3.3.10 What are the sub-categories of films? Notice the nyc_films data frame also has a column for SubCategoryName. Let’s see what’s going on there with a quick plot. # comments are really useful: R will ignore them, but they can explain to a reader # what is going on in the code # get the counts counts &lt;- nyc_films %&gt;% group_by(SubCategoryName) %&gt;% # group by SubCategoryName instead of Borough/Category summarize(count_of_permits = length(SubCategoryName)) # make the plot ggplot(counts, aes(x = SubCategoryName, y = count_of_permits, color=SubCategoryName, fill= SubCategoryName )) + geom_col() + theme_classic(base_size = 10) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ylab(&quot;Number of Film Permits&quot;) + xlab(&quot;Sub-category of film&quot;) + ggtitle(&quot;Number of Film permits in NYC by Sub-category&quot;) + theme(legend.position=&quot;none&quot;) I guess “episodic series” are the most common. Using a graph like this gave us our answer super fast. 2.3.3.11 facet_wrap() for categories by different boroughs Let’s see one more really useful thing about ggplot. It’s called facet_wrap(). It’s an ugly word, but you will see that it is very cool, and you can do next-level-super-hero graph styles with facet_wrap that other people can’t do very easily. Here’s our question. We know that films are made in different Boroughs, and that films are made in different Categories, but do different Boroughs have different patterns for the kinds of categories of films they request permits for? Are their more TV shows in Brooklyn? How do we find out? Watch, just like this: # get the counts counts &lt;- nyc_films %&gt;% group_by(Borough,Category) %&gt;% # group by two categories: Borough and Category summarize(count_of_permits = length(Category)) # make the plot ggplot(counts, aes(x = Category, y = count_of_permits, color=Category, fill= Category )) + geom_col() + theme_classic(base_size = 10) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ylab(&quot;Number of Film Permits&quot;) + xlab(&quot;Category of film&quot;) + ggtitle(&quot;Number of Film permits in NYC by Category and Borough&quot;) + theme(legend.position=&quot;none&quot;) + facet_wrap(~Borough, ncol=3) We did two important things. First we added Borough and Category into the group_by() function. This automatically gives separate counts for each category of film, for each Borough. Then we added facet_wrap(~Borough, ncol=3) to the end of the plot, and it automatically drew us 5 different bar graphs, one for each Borough! That was fast. Imagine doing that by hand. The nice thing about this is we can switch things around if we want. For example, we could do it this way by switching the Category with Borough, and facet-wrapping by Category instead of Borough like we did above. Do what works for you. ggplot(counts, aes(x = Borough, y = count_of_permits, color=Borough, fill= Borough )) + geom_col() + theme_classic(base_size = 10) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ylab(&quot;Number of Film Permits&quot;) + xlab(&quot;Borough&quot;) + ggtitle(&quot;Number of Film permits in NYC by Category and Borough&quot;) + theme(legend.position=&quot;none&quot;) + facet_wrap(~Category, ncol=5) 2.4 Gapminder dataset Gapminder is an organization that collects some really interesting worldwide data. We will actually be using their data for both of the assignments in this course. The Gapminder foundation makes cool visualization tools for looking at the data, built right into their website that you can play around with: https://www.gapminder.org/tools/. There is also an R library called gapminder which contains a small part of the data hosted on their website. When you install this library, it loads in some of the data from Gapminder directly into R, so we can easily play with it. You should have the Gapminder library installed if you followed all instructions of the Getting started guide, but if you don’t have it installed, you can install it by running this code: install.packages(&quot;gapminder&quot;) Once the library is installed, you need to load it and put the gapminder data into a data frame, like we do here: library(gapminder) # loads the library gapminder_df &lt;- gapminder 2.4.1 Look at the data You can look at the data frame in the environment tab to see what is in it, and/or you can use the head()/tail()/glimpse() functions again: head(gapminder_df) ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. tail(gapminder_df) ## # A tibble: 6 x 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Zimbabwe Africa 1982 60.4 7636524 789. ## 2 Zimbabwe Africa 1987 62.4 9216418 706. ## 3 Zimbabwe Africa 1992 60.4 10704340 693. ## 4 Zimbabwe Africa 1997 46.8 11404948 792. ## 5 Zimbabwe Africa 2002 40.0 11926563 672. ## 6 Zimbabwe Africa 2007 43.5 12311143 470. glimpse(gapminder_df) ## Rows: 1,704 ## Columns: 6 ## $ country &lt;fct&gt; Afghanistan, Afghanistan, Afghanistan, Afghanistan, Afghani… ## $ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia,… ## $ year &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997,… ## $ lifeExp &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.… ## $ pop &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 1… ## $ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134,… There are 1704 rows of data, and we see 6 columns: country, continent, year, life expectancy, population, and GDP per capita. We will show you how to graph some the data to answer a few different kinds of questions. Then you will form your own questions, and see if you can answer them with ggplot yourself. All you will need to do is copy and paste the following examples, and change them up a little bit. 2.4.2 Life expectancy How long are people living all around the world according to this data set? There are many ways we could plot the data to find out. The first way is with a histogram. We have many numbers for life expectancy in the column lifeExp. This is a big sample, full of numbers for 142 countries across many years. It’s easy to make a histogram in ggplot to view the distribution: ggplot(gapminder_df, aes(x=lifeExp))+ geom_histogram() Next, is a code block that adds more layers and settings to modify parts of the graph and make it more legible: ggplot(gapminder_df, aes(x = lifeExp)) + geom_histogram(color=&quot;white&quot;)+ theme_classic(base_size = 15) + ylab(&quot;Frequency count&quot;) + xlab(&quot;Life Expectancy&quot;) + ggtitle(&quot;Histogram of Life Expectancy from Gapminder&quot;) Compare this code to the previous code chunk to see what was changed/added. Either way, the histogram shows a wide range of life expectancies, from below 40 to just over 80. Histograms are useful, as they can show you what kinds of values happen more often than others. An important setting of histograms in ggplot is that of the bin size. That controls how wide or narrow the bars of your histogram are, by defining how the bars split across the range in the histogram. To do that, you need to set the bins= option in geom_histogram(). ggplot(gapminder_df, aes(x = lifeExp)) + geom_histogram(color=&quot;white&quot;, bins=50)+ theme_classic(base_size = 15) + ylab(&quot;Frequency count&quot;) + xlab(&quot;Life Expectancy&quot;) + ggtitle(&quot;Histogram of Life Expectancy from Gapminder&quot;) Same basic pattern, but now breaking up the range into 50 little equal sized bins, rather than 30, which is the default. You get to choose what you want to do. 2.4.2.1 Life expectancy time series plot We can see we have data for life expectancy and different years. So, does worldwide life expectancy change across the years in the data set? As we go into the future, are people living longer? Let’s look at this using something called a time series plot. We can set the x-axis to be year, and the y-axis to be life expectancy. Then we can use geom_point() to display a whole bunch of dots, and then look at them. Here’s the simple code: ggplot(gapminder_df, aes(y= lifeExp, x= year))+ geom_point() Whoa, that’s a lot of dots! Remember that each country is measured each year. So, the bands of dots you see, show the life expectancies for the whole range of countries within each year of the database. There is a big spread inside each year. But, on the whole it looks like groups of dots slowly go up over years. 2.4.2.2 One country, life expectancy by year I’m from The Netherlands, so maybe I want to know if life expectancy for Dutch people is going up over the years. To find out the answer for one country, we need to split the full data set into another smaller data set that only contains data for The Netherlands. In other words, we want only the rows where the word “Netherlands” is found in the country column. We will use the filter function for this: # filter rows to contain Netherlands smaller_df &lt;- gapminder_df %&gt;% # use the &quot;is equal to&quot; operator to filter filter(country == &quot;Netherlands&quot;) # plot the new data contained in smaller_df ggplot(smaller_df, aes(y= lifeExp, x= year))+ geom_point() Note that to filter, we are using the == (is equal to) operator. In the next section we will see it is often more useful to use the %in% operator instead. Either way, I would say things are looking good for Dutch people, their life expectancy is going up over the years! 2.4.2.3 Multiple countries, one plot What if we want to look at a few countries together? We can do this too. We just change how we filter the data so more than one country is allowed, then we plot the data. We will also add some nicer color options and make the plot look pretty. First, the code: # filter rows to contain countries of choice # create a vector using c() with countries of choice countries_of_choice &lt;- c(&quot;Netherlands&quot;,&quot;France&quot;,&quot;Brazil&quot;) smaller_df &lt;- gapminder_df %&gt;% # use the %in% operator for filtering with vector filter(country %in% countries_of_choice) # plot the new data contained in smaller_df ggplot(smaller_df, aes(y= lifeExp, x= year, group= country))+ geom_point() The code above contains two new concepts: We create a vector of countries_of_choice using c() (combine). A vector is simply a variable that holds multiple values. We use the %in% operator to filter the column country by our vector countries_of_choice. We need to use the %in% operator when we filter with a vector (i.e filtering for multiple values) as opposed to the == (is equal to) operator we use when filtering for a single value. Looking at the plot, we can now see three sets of dots, but which countries do they represent? Let’s color the dots, add a legend and make the graph prettier: ggplot(smaller_df,aes(y= lifeExp, x= year, group= country, color = country)) + geom_point()+ theme_classic(base_size = 15) + ylab(&quot;Life Expectancy&quot;) + xlab(&quot;Year&quot;) + ggtitle(&quot;Life expectancy by year for three countries&quot;) 2.4.2.4 geom_line() connecting the dots We might also want to connect the dots with a line, to make it easier to see the connection! Remember, ggplot2 draws layers on top of layers. So, we add in a new geom_line() layer. ggplot(smaller_df,aes(y= lifeExp, x= year, group= country, color = country)) + geom_point()+ geom_line()+ theme_classic(base_size = 15) + ylab(&quot;Life Expectancy&quot;) + xlab(&quot;Year&quot;) + ggtitle(&quot;Life expectancy by year for three countries&quot;) With the code examples above, you should be able to make your own graphs to answer questions about the Gapminder dataset. Switch to your lab template to complete the exercises below. 2.4.2.5 Graphing exercises Use the code from prior code blocks to solve the extra things we ask you to do for the following exercises: Make a histogram of GDP per capita in the Gapminder dataset. (hint: see the histogram of life expectancy in Section 2.4.2 for a code example) Make a time series plot of GDP per capita by year for Brazil, Canada, and Mexico using the code example in Section 2.4.2.4. Make sure to update the title and labels as well. Try to recreate time series plot below. The plot is of Life Expectancy by year for each continent. You should use the group_by() and summarise() functions. Have a look at grouping and summarizing the NYC film permits for a code example (Section 2.3.1). 2.5 Using numbers to describe data As we saw in the textbook, we can also describe our data using numbers. In order to do this, we need some numbers first. 2.5.1 Playing with numbers As we’ve seen previously, we can put multiple values in a variable using the c() function: my_numbers &lt;- c(1,2,3,4) There a few handy ways to generate numbers. For example, we can use seq() to generate a sequence of numbers. Here it is making the numbers from 1 to 100: one_to_one_hundred &lt;- seq(1,100,1) We can repeat things, using rep. Here’s making 10 5s, and 25 1s: rep(10,5) ## [1] 10 10 10 10 10 rep(1,25) ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 all_together_now &lt;- c(rep(10,5),rep(1,25)) 2.5.1.1 Sum Let’s play with the numbers 1 to 100. First, let’s use the sum() function to add them up: one_to_one_hundred &lt;- seq(1,100,1) sum(one_to_one_hundred) ## [1] 5050 2.5.1.2 Length We put 100 numbers into the variable one_to_one_hundred. We know how many numbers there are in there. How can we get R to tell us? We use length() for that: length(one_to_one_hundred) ## [1] 100 2.5.2 Central Tendency 2.5.2.1 Mean Remember the mean of some numbers is their sum, divided by the number of numbers. We can compute the mean like this: sum(one_to_one_hundred)/length(one_to_one_hundred) ## [1] 50.5 Or, we could just use the mean() function like this: mean(one_to_one_hundred) ## [1] 50.5 2.5.2.2 Median The median is the number in the exact middle of the numbers ordered from smallest to largest. If there are an even number of numbers (no number in the middle), then we take the number in between the two (decimal .5). Use the median function. There’s only 3 numbers here. The middle one is 2, that should be the median median(c(1,2,3)) ## [1] 2 2.5.2.3 Mode R does not a base function for the mode, which might surprise you. There’s a couple of reasons why, but we can workaround this omission. Firstly, you could install a library (like modeest) to import a function that can calculate the mode for you. Or you could write your own function. Below is an example of writing your own function, and then using it. Note I searched how to do this on Google, and am using the mode defined by this answer on stack overflow Remember, the mode is the most frequently occurring number in the set. # write a function to calculate the mode my_mode_function &lt;- function(x) { ux &lt;- unique(x) ux[which.max(tabulate(match(x, ux)))] } # apply to function to a vector of numbers my_mode_function(c(1,1,1,1,1,1,1,2,3,4)) ## [1] 1 Above 1 occurs the most, so the mode will be one. 2.5.3 Variation We often want to know how variable (i.e. different) the numbers in our data are. We are going to look at descriptive statistics to describe this such as the range, variance, the standard deviation, and a few others. First, let’s remind ourselves what variation looks like (it’s when the numbers are different). We will sample 100 numbers from a normal distribution (don’t worry about this yet, we will do more of this in later weeks), with a mean of 10, and a standard deviation of 5, and then make a histogram so we can see the variation around 10. sample_numbers &lt;- rnorm(100,10,5) # instead of using ggplot, we can also use a simpler function go generate a histogram # this is easier than ggplot, but more limited in what we can do with it hist(sample_numbers) Note: rnorm() generates a selection of 100 random numbers each time you run the function. As such, the numbers will slightly vary from student to student and the example descriptive statistics below. 2.5.3.1 Range The range is the minimum and maximum values in the set, we can use the range function: range(sample_numbers) ## [1] 1.004229 29.809107 Alternatively, when people refer to the range, they can refer to the difference between the minimum maximum values, which you could calculate as follows: diff(range(sample_numbers)) ## [1] 28.80488 2.5.3.2 var() = variance We can find the sample variance using var(): var(sample_numbers) ## [1] 26.67956 Note: the above variance calculation divides by \\(N-1\\) instead of \\(N\\). We will learn in later weeks why this is often preferred. If you would like to divide by \\(N\\), you could write your own function based on the following code: x_bar &lt;- mean(sample_numbers) var_n &lt;- sum((sample_numbers - x_bar)^2)/length(sample_numbers) 2.5.3.3 sd() = standard deviation We find the sample standard deviation using sd(): sd(sample_numbers) ## [1] 5.165226 Remember that the standard deviation is just the square root of the variance, see: sqrt(var(sample_numbers)) ## [1] 5.165226 2.5.4 Descriptives by conditions Sometimes you will have a single variable with some numbers, and you can use the above functions to find the descriptives for that variable. Other times (most often in this course), you will have a big data frame of numbers, with different numbers in different conditions. You will want to find descriptive statistics for each the sets of numbers inside each of the conditions. Fortunately, this is where R really shines, it does it all for you in one go. Let’s illustrate the problem. Here I make a date frame with 10 numbers in each condition. There are 10 conditions, each labeled, A, B, C, D, E, F, G, H, I, J. scores &lt;- rnorm(100,10,5) conditions &lt;- rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;,&quot;F&quot;,&quot;G&quot;,&quot;H&quot;,&quot;I&quot;,&quot;J&quot;), each =10) my_df &lt;- data.frame(conditions,scores) If you look at the my_df data frame, you will see it has 100 rows, there are 10 rows for each condition with a label in the conditions column, and 10 scores for each condition in the scores column. What if you wanted to know the mean of the scores in each condition? You would want to find 10 means. 2.5.4.1 group_by() and summarise() We can easily do everything all at once using the group_by and summarise function from the tidyverse library, as we’ve seen before: my_df %&gt;% group_by(conditions) %&gt;% summarise(means = mean(scores)) ## # A tibble: 10 x 2 ## conditions means ## * &lt;chr&gt; &lt;dbl&gt; ## 1 A 6.43 ## 2 B 9.86 ## 3 C 11.1 ## 4 D 12.1 ## 5 E 8.66 ## 6 F 11.0 ## 7 G 10.1 ## 8 H 10.2 ## 9 I 10.9 ## 10 J 8.87 The print out of this looks rather ugly. We can fix that by putting the results of our code into a new variable, then use knitr::kable() to print it out nicely when we knit the document. We can even caption the table by passing a caption argument. summary_df &lt;- my_df %&gt;% group_by(conditions) %&gt;% summarise(means = mean(scores)) knitr::kable(summary_df,caption = &quot;Means by condition.&quot;) Table 2.1: Means by condition. conditions means A 6.434879 B 9.858190 C 11.062653 D 12.063631 E 8.659427 F 10.961566 G 10.066427 H 10.159905 I 10.857577 J 8.868094 2.5.4.2 Multiple descriptives We can add more than one function, and we’ll get more than one summary returned. Let’s add the standard deviation: summary_df &lt;- my_df %&gt;% group_by(conditions) %&gt;% summarise(means = mean(scores), sds = sd(scores)) knitr::kable(summary_df) conditions means sds A 6.434879 5.915104 B 9.858190 6.175905 C 11.062653 4.060548 D 12.063631 3.796526 E 8.659427 5.264645 F 10.961566 3.492853 G 10.066427 6.586015 H 10.159905 5.114754 I 10.857577 4.373630 J 8.868094 3.872606 We’ll add the min and the max too: summary_df &lt;- my_df %&gt;% group_by(conditions) %&gt;% summarise(means = mean(scores), sds = sd(scores), min = min(scores), max = max(scores)) knitr::kable(summary_df) conditions means sds min max A 6.434879 5.915104 -0.860523 15.69073 B 9.858190 6.175905 -1.474144 20.48493 C 11.062653 4.060548 5.240914 18.93747 D 12.063631 3.796526 5.316051 16.63234 E 8.659427 5.264645 1.066329 19.31286 F 10.961566 3.492853 4.624445 16.97146 G 10.066427 6.586015 -3.836238 18.93662 H 10.159905 5.114754 3.051798 19.82208 I 10.857577 4.373630 2.274259 16.58054 J 8.868094 3.872606 3.236036 15.23494 2.5.5 Describing Gapminder with numbers Now that we know how to get descriptive statistics from R, we can do this with some real data. Let’s quickly ask a question about the Gapminder data: what are some descriptives for Life expectancy by continent? The code below was copied from the previous examples with just a few names changed: library(gapminder) gapminder_df &lt;- gapminder summary_df &lt;- gapminder_df %&gt;% group_by(continent) %&gt;% summarise(means = mean(lifeExp), sds = sd(lifeExp), min = min(lifeExp), max = max(lifeExp)) knitr::kable(summary_df) continent means sds min max Africa 48.86533 9.150210 23.599 76.442 Americas 64.65874 9.345088 37.579 80.653 Asia 60.06490 11.864532 28.801 82.603 Europe 71.90369 5.433178 43.585 81.757 Oceania 74.32621 3.795611 69.120 81.235 You should now be ready to complete the final exercises of this lab in your lab template. 2.5.5.1 Descriptive exercises What is the mean, standard deviation, minimum and maximum GDP per capita for all the gapminder data (across all the years and countries)? Print the values in a table with your own caption. What is the mean, median, variance, and length of the life expectancy variable for each continent in 2007? Print the values in a table with your own caption. Answer the following questions in your own words: Define the mode. Explain what would need to happen in order for a set of numbers to have two modes Define the median Define the mean Define the range When calculating the variance, explain what the difference scores represent Explain why the difference scores are squared when calculating the variance If one set of numbers had a standard deviation of 5, and another had a standard deviation of 10, which set of numbers would have greater variance? Explain why. When you have completed all exercises and are happy with your progress today, please knit your document and submit it to Canvas. If you finish before the time is up, you can help out your fellow students, start with the required readings of Week 3, or work on your assignment. If you haven’t done so already, make sure you’ve signed up for an assignment group. To be more exact: a tibble, but the difference with a data.frame is not important for now↩︎ Since we are using the group_by function, any of the columns would actually count up to the number of permits per borough. Instead of length(Borough) we could also summarize using n(). In fact, n() is the “tidy way” of doing it, but length() is used in this code example for illustrative purposes.↩︎ It is pretty neat to look and see all of the different things you can do with ggplot, it’s very powerful↩︎ "],["week-3-correlation-and-causation.html", " 3 Week 3: Correlation and Causation 3.1 Learning goals 3.2 Research methods: causality 3.3 Correlations in R 3.4 Real data", " 3 Week 3: Correlation and Causation In lecture and in the textbook, we have been discussing the idea of correlation. This is the idea that two things that we measure can be somehow related to one another. For example, your personal happiness, which we could try to measure say with a questionnaire, might be related to other things in your life that we could also measure, such as number of close friends, yearly salary, how much chocolate you have in your bedroom, or how many times you have said the word Nintendo in your life. Some of the relationships that we can measure are meaningful, and might reflect a causal relationship where one thing causes a change in another thing. Some of the relationships are spurious, and do not reflect a causal relationship. In this lab we will first discuss several research design concepts related to establishing causal relationships. In the second part of the lab you will learn how to compute correlations between two variables and answer some questions about the relationships between those variables. To get started, download the lab template here (right click: save as) or from Canvas. Copy the lab template to your lab folder and double click the Lab project file (not the .Rmd) to open RStudio. 3.1 Learning goals During this lab you will do the following: Discuss relationships between variables and how to establish causal relationships Discuss the possible meaning of correlations that you observe Learn how to compute and visualize Pearson’s \\(r\\) between two variables in R 3.2 Research methods: causality Discuss the research methods questions about causality in your breakout room and register the answers in the R Markdown template. 3.2.1 Question 1 Take a close look at the following two examples. Each example consists of a graph showing the fluctuation of two variables and a statement suggesting how to interpret such evidence. Are these statements correct? Explain your answer. 3.2.1.1 Example 1 There is a positive causal relationship between the number of people who drowned by falling into a pool and the number of films Nicolas Cage appeared in. Figure 3.1: Relationship between number of drownings and films Nicolage Cage appeared in. Illustration by Tyler Vigen. 3.2.1.2 Example 2 There is a positive causal relationship between per capita consumption of mozzarella cheese and civil engineering doctorates awarded. Figure 3.2: Relationship between mozerella consumption and civil engineering doctorates. Illustration by Tyler Vigen. 3.2.2 Question 2 For both scenarios described below, identify a possible confounder (variable Z) and describe its association with variables X and Y. The observation that when ice cream sales go up drownings of people in swimming pools increase, led the researcher to believe that ice cream consumption increases drownings. The observation that there is a very strong correlation between the use of social media and symptoms of depression, led the researcher to believe that excessive use of social media causes symptoms of depression. 3.2.3 Question 3 The case below presents an empirical observation that leads to a proposition. First, you are asked to identify the problem with this proposition. Then, you must provide a solution to overcome this issue. Your proposed solution should strengthen claims of a causal relationship between variables X and Y. Your proposed solution should satisfy the following: Clearly identify the source of variation in the independent variable (i.e. define groups) Indicate how you would use the variation in the independent variable to test against the proposed causal relationship There is enough variation in the independent variable (i.e. number of groups) such that the effect of interest can be identified in sufficient detail. Case Observation: We observe that people who exercise their hobbies regularly tend to be happier. Proposition: The more one exercises his or her hobbies, the happier he or she will be. Problem: Solution: 3.2.4 Question 4 In the case from the previous question: how would you test the empirical validity of competing explanations? For example, how could you test that exercising a hobby has a stronger effect for non-retirees than it has for retirees? 3.2.5 Question 5 In the previous question, you provided solutions to ambiguously formulated claims of causal relationships, in order to strengthen their empirical validity and to be able to draw meaningful conclusions. However, meaningful conclusions should be obtained from meaningful comparisons. According to the textbook, what does a meaningful comparison entail? 3.2.6 Question 6 According to the textbook, how can we obtain meaningful comparisons? 3.2.7 Question 7 The textbook discusses three different types of interventions (independent variables). Briefly describe what they are and provide an example of your own. 3.2.8 Question 8 Read the following two cases and decide whether or not the described experiment is internally valid. If not, identify the threat to internal validity by choosing from the options below. Selection bias Demand characteristics effect History effect Maturation effect Repeated testing effect Regression to the mean Differential attrition Non-response bias Experimenter bias Placebo effect 3.2.8.1 Case 1 It was the start of a new swim season, and the coach was already concerned about her swimmers’ lack of effort during the 20-minute dry-land portion of practice. She felt they didn’t take it seriously enough, and too often they would be talking or slacking off rather than doing the required stretches and exercises. She consulted with a sports psychologist. He suggested that she write the names of all 20 swimmers on individual slips of paper, scramble them, and then pick ten without looking. This would be used to assign swimmers to a Non-contingent Group (the first ten), who would have trainings on Monday, Wednesday and Friday, and a Contingent Group (the remainder), who would have training on Tuesday, Thursday, and Saturday. The coach thought it would be easier simply to assign the swimmers to groups on the basis of friendship, but she changed her mind after the psychologist explained the rationale for this slightly more cumbersome procedure. The intervention was that swimmers in the Contingent Group were told that if their dry-land productivity as a group for the day was 15% better than their average in the previous week then music would be played at the following practice. The productivity of the Non-contingent Group had no bearing on the playing of music. Then, each practice day for one week, observers unobtrusively recorded the swimmers’ productive behaviors during the dry-land training portion and calculated the percentage of one-minute intervals in which all swimmers in a group were being productive. Following this intervention, the dry-land productivity of the Contingent group was found to be higher than that of the Non-Contingent group so we can conclude that making the playing of music contingent on the swimmers’ dry-land productivity resulted in an increase in that productivity. 3.2.8.2 Case 2 Psychoanalysts at two different hospitals were asked to judge the well-being of a young man being interviewed on videotape. By the flip of a coin, psychoanalysts at a publicly-funded hospital were assigned to the Normal Group and psychoanalysts at a privately-funded hospital were assigned to the Abnormal Group. In the Normal Group, the doctors were told that the young man was a job applicant; in the Abnormal Group, the doctors were told that he was a patient. The mean adjustment rating by psychoanalysts in the Normal Group was 7 out of 8 compared to a 3.5 out of 8 rating by doctors in the Abnormal Group. We can conclude that the psychoanalysts’ ratings were affected by the label used to describe the young man. 3.3 Correlations in R In this part of the lab we will use R to explore correlations between two variables. Make sure you really follow along with the code examples given. 3.3.1 cor() for correlation R has the cor function for computing Pearson’s \\(r\\) between any two variables. In fact this same function can compute other versions of correlation as well (Spearman and Kendall), but we’ll skip those here. To use the function you just need two variables with numbers in them like this: x &lt;- c(1,3,2,5,4,6,5,8,9) y &lt;- c(6,5,8,7,9,7,8,10,13) cor(x,y) ## [1] 0.76539 3.3.1.1 Scatterplots Let’s take our toy example, and plot the data in a scatterplot using ggplot2. Let’s also return the correlation and print it on the scatter plot. Remember, ggplot2 wants the data in a data.frame, so we first put our x and y variables in a data frame. library(tidyverse) df &lt;- data.frame(x,y) ggplot(df, aes(x=x,y=y))+ geom_point()+ annotate(&quot;text&quot;, label= round(cor(x,y),2), x = 2, y = 12) Wow, we’re moving fast here. Dissect the code above and see if you understand each step. 3.3.1.2 Lots of scatterplots Before we move on to real data, let’s use some fake data first. Often we will have many measures of X and Y, split between a few different conditions, for example, condition A, B, C, or D. Let’s make some fake data for X and Y, for each condition A, B, C, and D. x &lt;- rnorm(40,0,1) # rnorm() generates a normally distributed random variable y &lt;- rnorm(40,0,1) condition &lt;-rep(c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;), each=10) df_all &lt;- data.frame(condition, x, y) We now have a data frame with 40 (random) observations over 4 different conditions: head(df_all) ## condition x y ## 1 A 1.19955455 -0.1248665 ## 2 A -1.71143716 -0.6168875 ## 3 A -1.00018053 0.2541530 ## 4 A 0.48148618 -1.1838535 ## 5 A -0.09775389 -0.9559277 ## 6 A -2.66701580 -1.2558958 glimpse(df_all) ## Rows: 40 ## Columns: 3 ## $ condition &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;B&quot;,… ## $ x &lt;dbl&gt; 1.19955455, -1.71143716, -1.00018053, 0.48148618, -0.097753… ## $ y &lt;dbl&gt; -0.12486652, -0.61688746, 0.25415301, -1.18385347, -0.95592… If we were to make a scatterplot of this dataframe to look at the relationship between the x and y variable, we could do the following: ggplot(df_all, aes(x = x, y = y))+ geom_point() Figure 3.3: All conditions on one scatterplot This visualizes the relationship between x and y over all conditions, but doesn’t allow us to see whether the relationship depends on the condition. If we would want to look at that, we need some way to visually discriminate the conditions on the scatterplot. You could do the following: ggplot(df_all, aes(x = x, y = y, group = condition, color = condition))+ geom_point() Figure 3.4: All conditions in one scatterplot, but conditions are differently colored This is a bit better, but it’s still not easy to discriminate between conditions. Better would be to plot the conditions as four separate scatterplots and the function facet_wrap() allows us to do just that: ggplot(df_all, aes(x = x, y = y))+ geom_point() + facet_wrap(~condition) Figure 3.5: Facet wrapped scatterplot 3.3.1.3 Computing the correlations all at once We’ve seen how we can make four graphs at once. facet_wrap() will always try to make as many graphs as there are individual conditions in the condition variable. In this case there are four, so it makes four. What we will do now is make a table of the correlations in addition to the scatter plot. We use functions from the tidyverse library to group by conditions and calculate the correlations (summarise()) and the function kable() from the knitr library to output a nice table: df_cor &lt;- df_all %&gt;% group_by(condition) %&gt;% summarise(correlation = cor(x, y)) knitr::kable(df_cor,caption = &quot;Correlation by condition.&quot;) Table 3.1: Correlation by condition. condition correlation A 0.4917795 B 0.7524485 C -0.2953659 D 0.4449637 OK, we are basically ready to turn to some real data and ask if there are correlations between interesting variables… but first some exercises. Complete the following exercises in your template file. 3.3.2 Correlation exercises Calculate the correlation between the following two variables: x &lt;- c(1,2,4,8,16,32,64,NA,256,512) y &lt;- c(1,2,2,2,5,6,5,8,7,4) What happens in exercise 1? What is NA? Copy/paste the code above and change it so you can still calculate the correlation between x and y, regardless of NA values. The following plot has two problems: the value of the correlation is printed with a very large amount of decimals and at a weird position. Try to fix the problems in the code below. x &lt;- c(1,2,4,8,16,32,64,128,256,512) y &lt;- c(1,2,2,2,5,6,5,8,7,4) corxy &lt;- cor(x,y) df &lt;- data.frame(x,y) ggplot(df, aes(x=x,y=y))+ geom_point()+ annotate(&quot;text&quot;, label = cor(x,y), x = 50, y = 2) Looking at the plot above, why is it probably not valid to summarise this data using Pearson’s \\(r\\)? 3.4 Real data Let’s take a look at some correlations in real data. We are going to look at responses to a questionnaire about happiness that was sent around the world, from the World Happiness Report. 3.4.1 Load the data We first load the data into a data.frame: # If you haven&#39;t already, this will load the tidyverse library and data library(tidyverse) whr_data &lt;- read_csv(&#39;data/WHR2018.csv&#39;) 3.4.2 Look at the data head(whr_data) tail(whr_data) glimpse(whr_data) You should be able to see that there is data for many different countries, across a few different years. There are lots of different kinds of measures, and each are given a name. You can find in the World Happiness Report what each of the measures represent. For now, I’ll show you some examples of asking questions about relationships between variables within this data, then you get to ask and answer your own question. 3.4.3 Example question #1 For the year 2017, does a countries measure for “Social support” correlate with that countries measure for “Healthy life expectancy at birth?” Let’s find out. We make the scatter plot and then calculate the correlation. We did something similar in the textbook, so the following should look familiar to you: ggplot(whr_data, aes(x=`Social support`, y=`Healthy life expectancy at birth`))+ geom_point()+ theme_classic() cor(whr_data$`Social support`, whr_data$`Healthy life expectancy at birth`) ## [1] NA We see a lot of dots on the scatterplot, but the correlation has a value of NA (meaning undefined or not available). This occurred because there are some missing data points in the data and we can’t calculate a correlation over missing data. We should remove all the rows with missing data first (i.e. wrangle with our data), then calculate the correlation. We do this in a couple of steps. First, we create our own data.frame with only the numbers we want to analyse. Then we select the columns we want to keep with select and use filter to remove the rows with NAs. smaller_df &lt;- whr_data %&gt;% select(country, `Social support`, `Healthy life expectancy at birth`) %&gt;% filter(!is.na(`Social support`), !is.na(`Healthy life expectancy at birth`)) cor(smaller_df$`Social support`, smaller_df$`Healthy life expectancy at birth`) ## [1] 0.5867593 Now we see the correlation is approximately 0.59. However, this is the correlation for all years in the dataset, not just 2017. So, we need to filter for the year 2017 as well: smaller_df &lt;- whr_data %&gt;% select(country, `Social support`, `Healthy life expectancy at birth`, `year`) %&gt;% filter(`year` == 2017) %&gt;% filter(!is.na(`Social support`), !is.na(`Healthy life expectancy at birth`)) ggplot(smaller_df, aes(x=`Social support`, y=`Healthy life expectancy at birth`))+ geom_point()+ theme_classic() cor(smaller_df$`Social support`, smaller_df$`Healthy life expectancy at birth`) ## [1] 0.7200551 Although the scatter plot shows the dots are everywhere, it generally shows as social support increases, life expectancy increases. Let’s add a best fit line, so the trend is more clear. We use the geom_smooth() function to do this. We can also annotate the correlation value on the graph and change the alpha value of the dots so they blend a bit: # Select variables of interest and filter for NAs and year == 2017 smaller_df &lt;- whr_data %&gt;% select(country, `Social support`, `Healthy life expectancy at birth`, `year`) %&gt;% filter(`year` == 2017) %&gt;% filter(!is.na(`Social support`), !is.na(`Healthy life expectancy at birth`)) # Calculate correlation corxy &lt;- cor(smaller_df$`Social support`, smaller_df$`Healthy life expectancy at birth`) # Plot the data with best fit line ggplot(smaller_df, aes(x=`Social support`, y=`Healthy life expectancy at birth`))+ geom_point(alpha=.5)+ geom_smooth(method=lm,se=FALSE)+ annotate(&quot;text&quot;,label=paste(&quot;Correlation:&quot;,round(corxy,2)), x=0.4,y=70) + theme_classic() 3.4.4 Example question #2 After all that work, we can now speedily answer more questions. For example, what is the relationship between positive affect in a country and negative affect in a country? I wouldn’t be surprised if there was a negative correlation here: when positive feelings generally go up, shouldn’t negative feelings generally go down? To answer this question, we just copy paste the last code block, and change the variables to be Positive affect, and Negative affect: # Select variables of interest and filter for NAs and year == 2017 smaller_df &lt;- whr_data %&gt;% select(country, `Positive affect`, `Negative affect`, `year`) %&gt;% filter(`year` == 2017) %&gt;% filter(!is.na(`Positive affect`), !is.na(`Negative affect`)) # Calculate correlation corxy &lt;- cor(smaller_df$`Positive affect`, smaller_df$`Negative affect`) # Plot the data with best fit line ggplot(smaller_df, aes(x=`Positive affect`, y=`Negative affect`))+ geom_point(alpha=.5)+ geom_smooth(method=lm,se=FALSE)+ annotate(&quot;text&quot;,label=paste(&quot;Correlation:&quot;,round(corxy,2)), x=0.45,y=0.45) + theme_classic() There we have it. As positive affect goes up, negative affect goes down. A negative correlation. Complete the exercises below in your lab template. 3.4.5 Theory exercises Answer the following questions in your own words: Explain the difference between a correlation of r = .3 and r = .7. What does a larger value of r represent? Explain the difference between a correlation of r = .5, and r = -.5. 3.4.6 Data exercise You should have all tools available to answer your own question about a relationship in the WHR dataset. This should be a different relationship (and about a different year) than the examples above. You should calculate Pearson’s \\(r\\) of this relationship, generate a scatterplot with a best fit line, print the correlation on the scatterplot, and print the mean and standard deviation of both variables in a table. Your code should include all steps, from loading the relevant libraries and data file, to generating all the required output. Use comments throughout your code to explain what your are doing. When you have completed all exercises and are happy with your progress today, please knit your lab template and submit it to Canvas. If you finish before the time is up, start with the required readings of Week 4, work on your assignment, or help out your fellow students. "],["week-4-chance-and-probability-theory.html", " 4 Week 4: Chance and Probability Theory 4.1 Learning goals 4.2 Correlation and random chance 4.3 Generating data with sample() and binom() 4.4 Normal distribution 4.5 z-scores", " 4 Week 4: Chance and Probability Theory In the textbook we learned that we can find correlations by chance alone, even when there is no true correlation between two variables. During the first part of this lab we are going to explore this phenomenon further. We will generate some random data and then look at the correlations we calculate from random data. This will serve to develop our intuitions about inferential statistics; the focus of the remainder of this course. In the second part of the lab we are going to dive deeper in generating simulated data, which is very useful in helping us understand real data, and calculate all sorts of things about probability distributions. To get started, download the lab template here (right click: save as) or from Canvas. Copy the lab template to your lab folder and double click the Lab project file (not the .Rmd) to open RStudio. 4.1 Learning goals During this lab you will do the following: Explore correlations and random chance Learn how to generate simulated data in R Calculate and work with z-scores in R 4.2 Correlation and random chance We saw in the textbook, that we can find correlations by chance alone, even when there is no true correlation between the variables. For example, suppose we randomly sampled some numbers into x and y. We know they shouldn’t be related, because we randomly sampled the numbers. However, chance alone will sometimes create correlations between x and y. You can demonstrate this to yourself by using the code below. Let’s look at 20 “experiments,” with 5 random numbers for x and y each. library(tidyverse) x&lt;-round(rnorm(5*20,5,5)) # rnorm draws random numbers from a normal distribution y&lt;-round(rnorm(5*20,5,5)) conditions&lt;-rep(1:20, each=5) # put data in data.frame all_df &lt;- data.frame(conditions, x, y) # plot 20 scatterplots with best fit line ggplot(all_df, aes(x=x,y=y))+ geom_point()+ geom_smooth(method=lm, se=FALSE)+ facet_wrap(~conditions)+ theme_classic() You can see that the slope of the blue line is not always flat. Sometimes it looks like there is a correlation, when we know in fact the data is completely random. You can keep re-doing this graph, by re-knitting your RMarkdown document, or by pressing the little green play button. This is basically you simulating 20 experiments each time you press the button. Complete the chance exercises below in your RMarkdown template. 4.2.1 Chance exercises For the first two exercises, you will be sampling random numbers from a normal distribution a 1000 times. We provide some hints and example code two answer the exercises below. Estimate the range (minimum and maximum) of correlations that could occur by chance between two normally distributed random variables with n = 10. Estimate the range (minimum and maximum) of correlations that could occur by chance between two normally distributed random variables with n = 25. Hint: to estimate the range of correlations that chance can produce we randomly sample x and y many times (like in the first example), save the correlation between x and y each time, then look at the smallest and largest correlation. How can you do this programmatically, without having to press the ‘play’ button hundreds of times? The answer is using a for loop. The code below shows how to repeat everything inside the for loop 5 times. The variable i is an index, that goes from 1 to 5. The saved_value variable starts out as an empty variable, and then we put a value into it (at index position i, from 1 to 5). In this code, we put the sum of the products of x and y into the saved_value variable. At the end of the simulation, the saved_value variable contains 5 numbers. The min() and max() functions are used to find the minimum and maximum values for each of the 5 simulations. Modify the following code to answer question 1 and 2. saved_value &lt;- c() # create empty variable for (i in 1:5) { # loop over code between {} 5 times x &lt;- rnorm(n=10, mean=0, sd=5) # generate normally distributed random variables y &lt;- rnorm(n=10, mean=0, sd=5) saved_value[i] &lt;- sum(x*y) # save the sum of the product of x*y in saved_value } min_value &lt;- min(saved_value) max_value &lt;- max(saved_value) print(min_value) ## [1] -118.9097 print(max_value) ## [1] 65.00331 What proportion of simulated correlations (from question 1 and 2) is smaller than -0.3 and larger than +0.3 for n = 10 and n = 25? To calculate the proportion use the smaller than (&lt;) and greater than (&gt;) operators on your vector of correlations like so: mean(saved_value &lt; 0.3) The histogram below shows the correlations produced by random chance for n = 10. The red lines are set at -0.3 and +0.3. The text indicates the proportion of correlations outside the red lines. Try to recreate the histogram below as closely as possible for n = 25. Remember, programming is an iterative process, you almost never get to the solution in the first go. Make liberal use of the help function in R, Google, the tidyverse website and/or the cheatsheets and ask help from your classmates / the tutor when stuck. There are multiple ways to get to the plot below. After completing these exercises, continue the lab by reading Section 4.3. 4.3 Generating data with sample() and binom() There are many ways to make R generate numbers for you. We already used normal distribution to generate numbers for our random correlations. In this next part, we will explore additional functions and distributions to generate random numbers and look at rnorm() in more detail. 4.3.1 sample() The sample function is like an endless gumball machine. You put the gumballs inside with different properties, say As and Bs, and then you let sample endlessly take gumballs out. Check it out: gumballs &lt;- c(&quot;A&quot;,&quot;B&quot;) sample_of_gumballs &lt;-sample(gumballs, 10, replace=TRUE) sample_of_gumballs ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; Here the sample function randomly picks A or B each time. We set it do this 10 times, so our sample has 10 things in it. We set replace=TRUE so that after each sample, we put the item back into the gumball machine and start again. Here’s another example with numbers: some_numbers &lt;- c(1,2,3,4,5,5,5,5) sample_of_numbers &lt;-sample(some_numbers, 20, replace=TRUE) sample_of_numbers ## [1] 5 5 5 3 5 5 5 3 5 5 5 5 2 5 4 4 4 3 5 5 Let’s do one more thing with sample. Let’s sample 1000 times from our some_numbers variable, and then look at the histogram: some_numbers &lt;- c(1,2,3,4,5,5,5,5) sample_of_numbers &lt;-sample(some_numbers, 1000, replace=TRUE) sample_df &lt;- data.frame(sample_of_numbers) ggplot(sample_df,aes(x=sample_of_numbers)) + geom_histogram() We are looking at lots of samples from our little gumball machine of numbers. Notice that as we put more 5s in, more 5s come out of in our big sample of 1000. 4.3.2 rbinom() You can think of the binomial distribution as a coin flipping distribution. Or dice roll distribution. Or anything you can define a number of “successes” for. For example, when flipping a coin, we could consider flipping heads a “success.” You use rbinom as follows: rbinom(n, size, prob). n gives the number of flips you want to make. size is the number of coins you want to flip at once. prob is the probability that defines how often a “success” happens. Here’s how we flip one coin 10 times using rbinom (assuming the coin is fair): coin_flips &lt;- rbinom(10,1,.5) coin_flips ## [1] 1 0 1 0 1 1 0 1 0 0 We get a bunch of 0s, and 1s. We can pretend 0 = tails, and 1 = heads. If you flip 10 coins, how many heads do you get? We can can do the above again, and then sum(coin_flips). coin_flips &lt;- rbinom(10,1,.5) sum(coin_flips) ## [1] 6 Alright, so we get the sum of the 1s (i.e. the successes), which tells us the number of heads. But, if you keep redoing the above, you’ll get different answers each time. 5 heads will be the most frequent answer, but you will get lots of other answers too. We could do this 1000 times over, saving the number of heads for each set of 10 flips. Then we could look at the distribution of those sums. That would tell us about the range of things that can happen when we flip a coin 10 times. We can do that in a for loop like this: number_of_heads&lt;-length(1000) # make an empty variable to save things in for(i in 1:1000){ number_of_heads[i] &lt;- sum(rbinom(10,1,.5)) } heads_df &lt;- data.frame(number_of_heads) ggplot(heads_df,aes(x=number_of_heads)) + geom_histogram(binwidth=1,fill=&quot;white&quot;,color=&quot;black&quot;) The histogram shows us the frequency observing different numbers of heads (for 10 flips) across the 1000 simulations. 5 happens the most, but 2 happens sometimes, and so does 8. All of the possibilities seem to happen sometimes, some more than others. 4.3.3 sample and binom() exercises Why are you unable to run the following bit of code? What could you do to fix this? Does this change the nature of your sampling procedure? sample(c(1:10), 20) Simulate the amount of sixes you expect to roll when rolling 1 dice 10,000 times. Also simulate the amount of fives and sixes you expect to roll when rolling 10 dice 10,000 times? (hint: use rbinom() to simulate 10,000 dice rolls and sum() to calculate the amount of successes) What is the probability of obtaining exactly 4 heads when flipping 10 fair coins? And what is the probability to obtain at most 4 heads (so 0, 1, 2, 3, or 4 heads)? Cf. the textbook when unsure how to use the binom() functions. What is the probability to obtain at least 4 heads (so 4, 5, 6, 7, 8, 9, or 10 heads) when flipping 10 fair coins? You can calculate this probability in two ways, either by using pbinom() with an additional argument, or by using your answers from question 3. 4.4 Normal distribution 4.4.1 rnorm() You can use rnorm() to sample numbers from a normal distribution. It’s similar to rbinom(), but now you are sampling from a normal distribution instead: sample_numbers &lt;- rnorm(10000,0,1) # sample n = 10,000 numbers with mean = 0 and sd=1 sample_df &lt;- data.frame(sample_numbers) ggplot(sample_df,aes(x=sample_numbers)) + geom_histogram(binwidth=0.1,fill=&quot;white&quot;,color=&quot;black&quot;) There it is, a bell-shaped normal distribution with a mean of 0, and a standard deviation of 1. Just by changing the arguments of the rnorm() function, you can sample numbers from normal distributions with any mean or standard deviation. The nice thing about R functions is that they are a bit like Legos, you can put them together and come up with different things. What if wanted to sample from a distribution that looked like a two-humped camel’s back? Just sample from rnorm twice like this… mix away. sample_numbers &lt;- ( c( rnorm(10000,25,7.5), rnorm(10000,50,7.5)) ) sample_df &lt;- data.frame(sample_numbers) ggplot(sample_df,aes(x=sample_numbers)) + geom_histogram(binwidth=1,fill=&quot;white&quot;,color=&quot;black&quot;) You can generate as many numbers under a certain distribution as your computer can handle. 4.4.2 Graphing the normal distribution “Wait, I thought we already graphed a normal distribution.” We sort of did. We sampled numbers and made histograms that looked like a normal distribution. But, a normal distribution is more of an abstract idea. It looks like this in the abstract: normal_dist &lt;- dnorm(seq(-4,4,.1), 0, 1) values &lt;-seq(-4,4,.1) normal_df &lt;-data.frame(values,normal_dist) ggplot(normal_df, aes(x=values,y=normal_dist))+ geom_line()+ theme_classic() A really nice shaped bell-like thing. This normal distribution has a mean of 0, and standard deviation of 1. The heights of the lines tell you roughly how likely each value is. Notice, it is centered on 0 (most likely that numbers from this distribution will be near 0), and it goes down as numbers get bigger or smaller (so bigger or smaller numbers get less likely). Notice the values don’t go much beyond -4 and +4. This is because those values don’t happen very often. Theoretically any value could happen, but really big or small values have really low probabilities. 4.4.3 Calculating the probability of specific ranges. We can use R to tell us about the probability of getting numbers in a certain range. For example, when you think about it, it should be obvious that you have a 50% probability of getting the number 0 or lower. Half of the distribution is 0 or lower, so you have a 50% probability. We can use the pnorm() function to confirm this: pnorm(0, mean = 0, sd= 1) ## [1] 0.5 Agreed, pnorm() tells us the probability of getting 0 or lower is 0.5. Well, what is the probability of getting a 2 or greater? That’s a bit harder to judge, but obviously less than 50%. Use R like this to find out: pnorm(2, mean = 0, sd= 1) ## [1] 0.9772499 That doesn’t seem quite right. R is telling us the probability is 0.977, while we know it should be smaller than 0.5. That’s because by default, pnorm() gives the probability “up to and including” (denoted: P[X ≤ x]). The figure below visualizes what that means: So, to get the probability of getting a 2 or greater, we have to take \\(1 - 0.9772499 = 0.0227501\\): Or, we could use the lower.tail argument: pnorm(2, mean = 0, sd= 1,lower.tail = FALSE) ## [1] 0.02275013 So, the probability of getting a 2 or greater is .0227 (not very probable) 4.4.4 norm() exercises Complete the following exercises in your RMarkdown template. Run the following bit of code, which samples 20 random numbers from a normal distribution, a couple of times and look at the results. What do you think the function set.seed() does? When do you think this could be useful? set.seed(123) some_numbers &lt;- rnorm(20,50,25) # 20 numbers, mean = 50, s.d. = 25 print(some_numbers) ## [1] 35.9881088 44.2455628 88.9677079 51.7627098 53.2321934 92.8766247 ## [7] 61.5229051 18.3734691 32.8286787 38.8584507 80.6020449 58.9953457 ## [13] 60.0192863 52.7670679 36.1039716 94.6728284 62.4462620 0.8345711 ## [19] 67.5338975 38.1802148 Suppose the mean of a normal distribution is 25 (\\(\\mu = 25\\)) and the standard deviation is 3 (\\(\\sigma=3\\)). Calculate the probability of obtaining a value between 22 and 28 using R. (hint: pnorm()) Based on what you know about the standard normal distribution, could you answer question 2 without calculations? Use R to calculate the probability of obtaining a value higher than 29.5 for the normal distribution with \\(\\mu = 25\\) and \\(\\sigma=3\\). (hint: pnorm()) Continue the lab by reading section 4.5 of the lab manual. 4.5 z-scores A normal distribution with mean = 0 and the standard deviation = 1 is called the standard normal distribution. Often, we are not dealing with a normal distribution exactly like this. For example, someone might say, I got a number, it’s 545. It came from a normal distribution with mean = 600, and standard deviation = 25. Does the number 545 happen a lot or not? The numbers don’t tell you right away. But if we were talking about the standard normal distribution with mean = 0 and standard deviation = 1, and I told I got a number -2.2 from that distribution, you would know directly that -2.2 doesn’t happen a lot. z-scores are a way of transforming one set of numbers into the standard normal distribution. To calculate z-scores we take the following steps: First get some numbers: some_numbers &lt;- rnorm(20,50,25) # 20 numbers, mean = 50, s.d. = 25 Calculate the mean and standard deviation: my_mean &lt;- mean(some_numbers) my_sd &lt;-sd(some_numbers) print(my_mean) ## [1] 48.71857 print(my_sd) ## [1] 20.74847 Subtract the mean from your numbers: differences&lt;-some_numbers-my_mean print(differences) ## [1] -25.4141636 -4.1679438 -24.3686822 -16.9408517 -14.3445527 -40.8859037 ## [7] 22.2261051 5.1157570 -27.1719944 32.6268021 11.9430346 -6.0953580 ## [13] 23.6595706 23.2347662 21.8209561 18.4974354 15.1293704 -0.2663637 ## [19] -6.3676376 -8.2303460 Divide each number by the standard deviation: z_scores&lt;-differences/my_sd print(z_scores) ## [1] -1.22486947 -0.20087961 -1.17448110 -0.81648691 -0.69135482 -1.97055060 ## [7] 1.07121675 0.24656072 -1.30959047 1.57249219 0.57561046 -0.29377390 ## [13] 1.14030452 1.11983051 1.05169005 0.89150854 0.72918016 -0.01283775 ## [19] -0.30689677 -0.39667249 Done. Now you have converted your original numbers into what we call standardized scores (or z-scores). They are standardized to have the same (assumed) properties of a normal distribution with mean = 0, and standard deviation = 1. 4.5.1 z-score exercises Complete the following exercises in your RMarkdown template. Right click and download this SPSS file containing 49 students’ exam grades (let’s say it’s the final exam for a statistics class). You have likely never worked with an SPSS (.sav) file before, but luckily a library that can load SPSS files was installed when installing the tidyverse. To load this package (it is not loaded by default with library(tidyverse)), use library(haven). Make sure the .sav is in your lab directory so you can load it into R. Once you have successfully loaded the data into R: Create a table containing the mean and standard deviation for this sample of scores. Also produce a histogram of the grades. How does the distribution of grades look when you plot your histogram with a binwidth of 5? Transform each student’s score into a Z-score. Now, plot the frequency histogram of this Z-score distribution with a binwidth of 0.5. Compare it to the raw score distribution. How are they the same? How are they different? Imagine you are a student in this class who received a 90 on this exam. However, the Professor has decided to grade on a curve, such that only the top 10% of the class receives an A. Calculate the z-score that corresponds to a raw score of 90 on this exam. Will you get an A with this grade? Why or why not? When you have completed all exercises and are happy with your progress today, please knit your document and submit it to Canvas. If you finish before the time is up, start with the required readings of Week 5, work on your assignment, or help out your fellow students. "],["week-5-estimation-and-sampling-theory.html", " 5 Week 5: Estimation and Sampling Theory 5.1 Learning goals 5.2 Collecting some data 5.3 Statistical theory: sampling and estimation 5.4 Stroop data", " 5 Week 5: Estimation and Sampling Theory In the textbook we learned that we can use various concepts from sampling theory (e.g. the law of large numbers, sampling distributions and the central limit theorem) to estimate something about a population from a sample. In the first part of the lab we are going to discuss sampling theory and sampling distributions in more detail. During the second part of the lab we are going to use R to estimate things about a population using our own data data. You will notice this lab contains fewer “ready-made” code examples for you to copy/paste and adjust than in previous weeks. As we progress through the course, we expect you to be able to code more “from scratch” (with the help of a few hints), based on what you’ve learned in previous weeks and the resources available to you. To get started, download the lab template here (right click: save as) or from Canvas. Copy the lab template to your lab folder and double click the Lab project file (not the .Rmd) to open RStudio. 5.1 Learning goals During this lab you will do the following: Discuss various concepts related to sampling theory, sampling distributions and estimation Work with real data to estimate population parameters from a sample using R 5.2 Collecting some data Make sure you have completed the experiment and have registered your reaction times in the online spreadsheet provided by your tutor. Completing the experiment should take approximately 5 minutes. Your tutor will compile an excel file with the experimental data to complete the exercises at the end of this lab. 5.3 Statistical theory: sampling and estimation 5.3.1 Question 1 What is the difference between a population and a sample? Explain in your own words. 5.3.2 Question 2 Identify what type of sampling is conducted in the two cases below. Why is the sampling strategy in case 2 preferred when your population is not homogeneous? 5.3.2.1 Case 1 Suppose a researcher is interested in the average annual income of people living in The Netherlands. The researcher contacts the Belastingdienst (the Dutch tax authorities) to obtain a sample of income data. The Belastingdienst agrees to share 1,000 (anonymized) tax returns. The tax returns are selected completely at random and each member of the population has an equal probability of being sampled. 5.3.2.2 Case 2 You work for a polling organization in the USA and are tasked with surveying the popularity of the current president. You have resources to survey 500 participants. Instead of selecting 500 participants completely at random from the population, you make an effort to (randomly) sample 100 participants from rural areas and 400 participants from urban areas (the approximate ratio between rural inhabitants and urban inhabitants in the USA). 5.3.3 Question 3 Identify the source of sampling bias in the following two cases. Explain whether sampling bias could be a threat to the validity of each study. 5.3.3.1 Case 1 The course coordinator of a large course at EUC is investigating optical illusions. During a lecture, they include a slide containing a Ponzo illusion (see image below). Afterwards, they ask students whether the yellow lines are of equal length or not and they register the answers. 5.3.3.2 Case 2 A capstone student wants to investigate the use of recreational drugs by students in The Netherlands. They send out a survey about recreational drug use to 30 of their friends currently studying at EUC. They also ask their friends to forward the survey to anyone they know currently studying in The Netherlands. 5.3.4 Question 4 What is the difference between a population parameter and a sample statistic? Explain in your own words. 5.3.5 Question 5 In the example below we have R toss a fair coin ten times and calculate the mean amount of heads. The mean amount of heads (i.e. the proportion of heads) is calculated by: \\(\\frac{\\text{number of heads}}{\\text{number of flips}}\\) set.seed(123) theta &lt;- 0.5 # fair coin so P(Heads) = 0.5 N &lt;- 10 # number of flips flips &lt;- rbinom(n = N, size = 1, prob = theta) print(flips) ## [1] 0 1 0 1 1 0 1 1 1 0 print(mean(flips)) ## [1] 0.6 In this example that works out to be: \\(\\frac{\\text{6 heads}}{\\text{10flips}} = 0.6\\) In the long run, what do you expect the proportion of heads to be? Adjust the code above to flip 100 coins and 10,000 coins respectively. What is the proportion of heads for 100 and 10,000 coins? Explain why the proportion of heads gets closer and closer to \\(0.5\\) as the sample size increases. 5.3.6 Question 6 What is the difference between the distribution of your sample and the sampling distribution of the sample mean? How does a sampling distribution relate to the population? Explain in your own words. 5.3.7 Question 7 Consider the following two distributions. The first distribution is the sampling distribution of the sample mean of rolling one die 100,000 times. Of course, the sample mean of rolling one die is just the value of a single roll. # set parameters of imaginary dice rolling experiment n_rolls &lt;- 100000 n_dice &lt;- 1 # sample 100,000 dice rolls (single die) mean_rolls &lt;- replicate(n_rolls, mean(sample(1:6, size = n_dice, replace = TRUE))) # plot histogram of sample means # sample mean of 1 die is simply the value of a single roll dice_rolls &lt;- data.frame(mean_rolls) ggplot(dice_rolls,aes(x=mean_rolls)) + geom_histogram(binwidth = 1,fill=&quot;white&quot;,color=&quot;black&quot;) The second distribution is the sampling distribution of the sample mean of rolling four dice 100,000 times. Here, we have R roll 4 dice 100,000 times and calculate the mean of each sample of 4 dice. # set parameters of imaginary experiment n_rolls &lt;- 100000 n_dice &lt;- 4 # sample 100,000 dice rolls (four dice) mean_rolls &lt;- replicate(n_rolls, mean(sample(1:6, size = n_dice, replace = TRUE))) # plot histogram of sample means dice_rolls &lt;- data.frame(mean_rolls) ggplot(dice_rolls,aes(x=mean_rolls)) + geom_histogram(binwidth = 0.25,fill=&quot;white&quot;,color=&quot;black&quot;) Why does the shape of the second distribution differ from the first distribution? What is the relationship between the sample size (i.e. the amount of dice rolled per sample) and the standard deviation of the sampling distribution? 5.3.8 Question 8 Suppose we have information about the height of all Dutch males over 18 years of age. Height is normally distributed; the mean height and standard deviation of the population is as follows: \\[\\mu = 180 cm\\] \\[ \\sigma = 10 cm\\] What is the standard deviation of the sampling distribution of the sample mean when samples of size 16 are randomly drawn from the population? What is the standard deviation of the sampling distribution of the sample mean when samples of size 100 are randomly drawn from the population? 5.3.9 Question 9 Suppose you collect the mean reaction time in a sample of 9 participants with a mean of 200ms (\\(\\bar{X} = 200 \\text{ ms}\\)). You know the true population standard deviation is equal to 25 ms (\\(\\sigma = 25 \\text{ ms}\\)). What is the 95% confidence interval for the mean of this sample? What happens to the confidence interval of the mean if the sample size increases to 25? 5.4 Stroop data Let’s have a look at the data from our experiment and estimate something about the population based on our sample15. In a typical Stroop experiment, subjects name the color of words as fast as they can. The trick is that sometimes the color of the word is the same as the name of the word, and sometimes it is not, like you’ve probably noticed. Here is an example of the what you saw: Congruent trials occur when the color and word match. So, the correct answers for each of the congruent stimuli shown would be to say, red, green, blue and yellow. Incongruent trials occur when the color and word mismatch. The correct answers for each of the incongruent stimuli would be: blue, yellow, red, green. What happens is that people are faster to name the color of the congruent items compared to the color of the incongruent items. This difference (incongruent reaction time - congruent reaction time) is called the Stroop effect. The data your tutor provided can be loaded as follows: library(readxl) # Depending where you have placed and how you have named your file df_stroop_wide &lt;- read_xlsx(&quot;data/stroop.xlsx&quot;) Having a quick look at the dataframe will reveal the data is formatted in a so-called “wide” format. That is, each row are the repeated responses of a single participant and each response is in a separate column (variable). head(df_stroop_wide) ## # A tibble: 6 x 3 ## Participant Congruent Incongruent ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1200 1500 ## 2 2 1100 1350 ## 3 3 900 1000 ## 4 4 555 1111 ## 5 5 1200 1500 ## 6 6 1100 1350 Data entered into excel is often in the wide format, but a lot of functions in R require data to be in a so-called “long” format. In the long format, each variable forms a column, each observation forms a row, and each cell is a single measurement. To give a concrete example, compare the table of the average yearly temperature in three countries below: Table 5.1: Average temperature in three countries for 1995-1997 in wide format Country 1995 1996 1997 Sweden 5 12 4 Denmark 5 7 8 Norway 3 11 9 with the following table: Table 5.2: Average temperature in three countries for 1995-1997 in long format Country Year Average temperature Sweden 1995 5 Denmark 1995 5 Norway 1995 3 Sweden 1996 12 Denmark 1996 7 Norway 1996 11 Sweden 1997 4 Denmark 1997 8 Norway 1997 9 Both tables contain the same data, but Table 5.1 is in wide format, while Table 5.2 is in long format. There are various advantages working with data in a long format (explained here), but what is important to know is that most operations on your data are easier in the long format, while some operations are easier in the wide format. Furthermore, tidyverse functions like ggplot() expect data in the long format, so it’s important to learn how to go from wide format data to long format data (and vice versa). To functions to switch between data formats are pivot_longer() (wide to long) and pivot_wider() (long to wide). To change the format of our data from wide to long, we use the pivot_longer() function as follows: df_stroop_long &lt;- df_stroop_wide %&gt;% pivot_longer(cols = c(Congruent,Incongruent),names_to = &quot;Condition&quot;,values_to = &quot;RT&quot;) head(df_stroop_long) ## # A tibble: 6 x 3 ## Participant Condition RT ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Congruent 1200 ## 2 1 Incongruent 1500 ## 3 2 Congruent 1100 ## 4 2 Incongruent 1350 ## 5 3 Congruent 900 ## 6 3 Incongruent 1000 Our data is now in the long format, like Table 5.2. Let’s perform some operations on our data. The reaction times in our Stroop experiment are given in milliseconds, which is how reaction times are usually represented, but suppose we want to convert them to seconds instead. The mutate() function allows us to do that easily in long format data as follows: df_stroop_long_sec &lt;- df_stroop_long %&gt;% mutate(RT_sec = RT / 1000) head(df_stroop_long_sec) ## # A tibble: 6 x 4 ## Participant Condition RT RT_sec ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Congruent 1200 1.2 ## 2 1 Incongruent 1500 1.5 ## 3 2 Congruent 1100 1.1 ## 4 2 Incongruent 1350 1.35 ## 5 3 Congruent 900 0.9 ## 6 3 Incongruent 1000 1 Note: mutate() creates an additional variable (RT_sec) with the reaction times in seconds and preserves the variable RT. Use transmute() if you would like to drop the existing reaction time variable (RT) from your dataframe. To pivot back from the long format to the wide format, we can use pivot_wider() as follows: df_stroop_wide_sec &lt;- df_stroop_long_sec %&gt;% pivot_wider(names_from = Condition, values_from = c(RT,RT_sec)) head(df_stroop_wide_sec) ## # A tibble: 6 x 5 ## Participant RT_Congruent RT_Incongruent RT_sec_Congruent RT_sec_Incongruent ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1200 1500 1.2 1.5 ## 2 2 1100 1350 1.1 1.35 ## 3 3 900 1000 0.9 1 ## 4 4 555 1111 0.555 1.11 ## 5 5 1200 1500 1.2 1.5 ## 6 6 1100 1350 1.1 1.35 Now, let’s perform an operation on the wide format data. Suppose we want to calculate the difference between the Congruent and Incongruent condition in seconds. This can be achieved in wide format data with mutate() as follows: df_stroop_wide_sec_diff &lt;- df_stroop_wide_sec %&gt;% mutate(RT_sec_diff = RT_sec_Congruent - RT_sec_Incongruent) head(df_stroop_wide_sec_diff) ## # A tibble: 6 x 6 ## Participant RT_Congruent RT_Incongruent RT_sec_Congruent RT_sec_Incongru… ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1200 1500 1.2 1.5 ## 2 2 1100 1350 1.1 1.35 ## 3 3 900 1000 0.9 1 ## 4 4 555 1111 0.555 1.11 ## 5 5 1200 1500 1.2 1.5 ## 6 6 1100 1350 1.1 1.35 ## # … with 1 more variable: RT_sec_diff &lt;dbl&gt; Notice that the particular expression RT_sec_Congruent - RT_sec_Incongruent in mutate() only made sense in the context of wide format data. We could have achieved the above with different operations on the long format data, but the lesson is that you should pay attention to the format of your data: not all data you import is in the long format, while some functions require it, and the format of your data dictates how and which operations you can perform. Complete the Stroop data exercises below in your lab template. 5.4.1 Stroop data exercises Calculate the mean and standard deviation of the Stroop effect (i.e. the difference between the incongruent and congruent condition). There are several ways to achieve this and you can use either the wide format or long format data. Of course, there are differences how to approach this based on the format of your data. Pick the option that suits you most. Calculate the 95% confidence interval of the Stroop effect (hint: use qt() with the appropriate degrees of freedom to find the correct value for a 95% confidence interval). Why do you need to use a t-distribution for calculating the confidence interval in this particular instance? This next exercise is combining everything we have learned so far. Have a look at the following paper. The authors of this paper investigated the Stroop effect in two different conditions and the main results are summarized in Figure 1. Adjust/complete the code example below to recreate Figure 1 of the paper linked. We only have two conditions, so your figure will only include two bars + errorbars (congruent and incongruent). df_summary &lt;- df_stroop_long %&gt;% group_by(...) %&gt;% summarise(n = ..., meanRT = ..., sdRT = ...) %&gt;% mutate(se = ...) ggplot(df_summary , aes(x=... ,y= ...)) + geom_col() + geom_errorbar(aes(ymin = ... , ymax = ... )) When you have completed all exercises and are happy with your progress today, please knit your document and submit it to Canvas. If you finish before the time is up, start with the required readings of Week 6, work on your assignment, or help out your fellow students. Example adapted from Lab 10 in “Answering questions with data” by Matthew Crump↩︎ "],["week-6-hypothesis-testing.html", " 6 Week 6: Hypothesis Testing 6.1 Learning goals 6.2 Statistical theory: hypothesis testing 6.3 \\(t\\)-tests in R 6.4 Stroop data", " 6 Week 6: Hypothesis Testing You’ve made it to the final lab of this course, yay! The lab this week will be all about hypothesis testing, one of the most ubiquitous elements of statistical theory, and the conclusion of your journey into inferential statistics (for now…). Almost any quantitative research paper you read will feature a hypothesis test in one form or another. Moreover, the quantitative research you are going to conduct in other courses or capstone will likely rely heavily on the framework of hypothesis testing. During this lab we will first discuss the theories at the foundation of hypothesis testing. In the second part of the lab we are going to practice conducting null hypothesis significance tests (NHSTs) in R. To get started, download the lab template here (right click: save as) or from Canvas. Copy the lab template to your lab folder and double click the Lab project file (not the .Rmd) to open RStudio. 6.1 Learning goals During this lab you will do the following: Discuss various concepts related to hypothesis testing Conduct one-sample, paired-samples and independent samples \\(t\\)-tests in R 6.2 Statistical theory: hypothesis testing 6.2.1 Question 1 What is a research hypothesis? What is a statistical hypothesis? What is the difference between the two? Explain in your own words. 6.2.2 Question 2 What is the null and alternative hypothesis of the following cases: 6.2.2.1 Case 1 You flip a coin 50 times and count the number of heads flipped (\\(X = 12\\)). Suppose you want to conduct a hypothesis test to determine if the underlying probability of flipping heads for this particular coin is equal to \\(0.30\\). 6.2.2.2 Case 2 A random survey of 75 death row inmates revealed that the mean length of time on death row is 17.4 years with a standard deviation of 6.3 years. Suppose you want to conduct a hypothesis test to determine if the mean time on death row is 15 years.16 6.2.2.3 Case 3 You are a researcher investigating the effect of age on working memory. You have collected data on how many random digits can be remembered for a brief period of time in a group of 20 young adults (\\(\\bar{X} = 7.2\\) digits) and 20 elderly participants (\\(\\bar{X} = 6.9\\) digits). The standard deviation is 1 digit in both groups. Suppose you want to conduct a hypothesis test to determine if young adults have a better working memory than elderly. 6.2.2.4 Case 4 A course coordinator is interested if their course about statistics actually has an effect on the statistical ability of students. The course coordinator has 200 students take a quiz about statistics before the course starts (mean pre-course quiz grade = 50 points) and has the same students take a quiz after completion of the course (mean post-course quiz grade = 75 points). The course coordinator made sure the questions were different between the quizzes, but equivalent in difficulty. The standard deviation of the difference is 10 points. Suppose the course coordinator wants to conduct a hypothesis test to determine if the course had any effect on the statistical ability of students (i.e. quiz grades getting better or worse). 6.2.3 Question 3 What is the appropriate statistical test to conduct for each of the statistical hypotheses stated in Question 2? Explain why a particular statistical test is appropriate. Assume the population standard deviation is unknown for case 2, 3 and 4. 6.2.4 Question 4 What is meant by the critical region for a test statistic? What does it mean if the test statistic of your sample falls within the critical region? Explain in your own words. 6.2.5 Question 5 What is the meaning of the p-value of a statistical test? Explain in your own words. 6.2.6 Question 6 The following is the output of the statistical tests which tested the hypotheses of the cases stated in Question 2. Comment on the results of each statistical test. Do you reject or retain the null hypothesis in each case? 6.2.6.1 Case 1 &gt; binom.test(X,N,theta) Exact binomial test data: X and N number of successes = 12, number of trials = 50, p-value = 0.4407 alternative hypothesis: true probability of success is not equal to 0.3 95 percent confidence interval: 0.1306099 0.3816907 sample estimates: probability of success 0.24 6.2.6.2 Case 2 &gt; t.test(time_deathrow, mu = 15) One Sample t-test data: time_deathrow t = 3.2991, df = 74, p-value = 0.001493 alternative hypothesis: true mean is not equal to 15 95 percent confidence interval: 15.9505 18.8495 sample estimates: mean of x 17.4 6.2.6.3 Case 3 &gt; t.test(young_adults,elderly,alternative = &quot;greater&quot;) Welch Two Sample t-test data: young_adults and elderly t = 0.94868, df = 38, p-value = 0.1744 alternative hypothesis: true difference in means is greater than 0 95 percent confidence interval: -0.2331456 Inf sample estimates: mean of x mean of y 7.2 6.9 6.2.6.4 Case 4 &gt; t.test(pre,post,paired =T) Paired t-test data: pre and post t = -80.58, df = 199, p-value &lt; 2.2e-16 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -25.6118 -24.3882 sample estimates: mean of the differences -25 6.2.7 Question 7 What does it mean if the result of a statistical test is significant? Does statistical significance say anything about whether the result is important? 6.2.8 Question 8 What type of error is committed in the following cases? Which type of error has a greater consequence in this particular case?17 6.2.8.1 Case 1 You are a beginning rock climber and have bought some second hand climbing equipment on Marktplaats. Assume your null hypothesis about the safety of the rock climbing equipment is: the climbing equipment is safe. What type of error do you commit if you think the rock climbing equipment is not safe, when in fact it is safe? 6.2.8.2 Case 2 You are a beginning rock climber and have bought some second hand climbing equipment on Marktplaats. Assume your null hypothesis about the safety of the rock climbing equipment is: the climbing equipment is safe. What type of error do you commit if you think the rock climbing equipment is safe, when in fact it is not safe? 6.2.9 Question 9 What does the power of a statistical test refer to? Explain in your own words. 6.3 \\(t\\)-tests in R I’ll be honest. Conducting a \\(t\\)-test in R is sometimes slightly bothersome. Part of that has to do with the fact that the t.test() function was not really written with “tidy data” in mind (which we have become used to over the past couple of weeks), but also because t.test() can do more than just one thing. The function can actually do several different \\(t\\)-tests, so to use it you have to be quite specific in setting its arguments, depending on which \\(t\\)-test you want to perform. How to use t.test() is actually explained quite extensively in the textbook, so I will not go over all the details here, but I will highlight some important tips to succesfully conduct \\(t\\)-tests in R. Remember the three types of \\(t\\)-test we can conduct are: the one-sample \\(t\\)-test, the paired samples \\(t\\)-test and the independent samples \\(t\\)-test. I will focus on using the independent samples \\(t\\)-test here and how to use to other \\(t\\)-tests should follow from that. More details on how to use the different \\(t\\)-tests are given in the relevant sections of the textbook. Suppose we want to compare the mean grade of one group of students to a different group of students. Maybe the two groups were instructed using a different method (a “classic” method and a “modern” method) and we have some reason to believe the instruction method has an effect on the exam grade. We conduct an independent samples \\(t\\)-test as follows: set.seed(123) classic &lt;- rnorm(n = 10, mean = 70, sd = 7) # generate 10 grades for the classic group modern &lt;- rnorm(n = 10, mean = 75, sd = 7) # generate 10 grades for the modern group results &lt;- t.test(classic,modern) print(results) ## ## Welch Two Sample t-test ## ## data: classic and modern ## t = -1.9029, df = 17.872, p-value = 0.07329 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -12.4973418 0.6213934 ## sample estimates: ## mean of x mean of y ## 70.52238 76.46035 I want to briefly explain some of the arguments t.test() can take before having a detailed look at the results. All arguments were set at their default in the example above (i.e. they were not explicitly changed from the default setting), but sometimes you have to change an argument from its default depending on the \\(t\\)-test you are conducting. The most common arguments to change are: alternative: specify which alternative hypothesis to test. By default you test a two-sided hypothesis (“two.sided”), but you can also test a one-sided hypothesis (“greater” or “less”). mu: specify the value of mu for the null hypothesis (one-sample \\(t\\)-test) or the difference between groups (independent or paired-samples \\(t\\)-test). The value of mu is 0 by default. paired: indicate whether the groups are paired or not (not paired by default, i.e. independent) conf.level: set the confidence level of the confidence interval (0.95 by default) With that out of the way, let’s focus on the results of the independent samples \\(t\\)-test above. The \\(t\\)-test tested whether the difference between groups was equal to 0 or not (two-sided). The output reveals we shouldn’t reject the null hypothesis, because our p-value is not smaller than the value of \\(\\alpha\\) commonly used (0.05). We saved the results of the \\(t\\)-test in a variable called results. If we want to select a specific result of the \\(t\\)-test (for example the confidence interval), we can do that as follows: results$conf.int ## [1] -12.4973418 0.6213934 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 The output of a \\(t\\)-test doesn’t look very pretty and you might want to make it a bit more aesthetically pleasing before you incorporate this result in a research paper. You can do this by putting the results of the \\(t\\)-test in a dataframe and printing it as a table as follows: results_df &lt;- data.frame(results$statistic, results$parameter, results$p.value, results$conf.int[1], results$conf.int[2]) knitr::kable(results_df,col.names = c(&quot;Statistic&quot;,&quot;d.f.&quot;,&quot;p&quot;,&quot;CI lower&quot;,&quot;CI upper&quot;)) Statistic d.f. p CI lower CI upper t -1.902867 17.87244 0.0732939 -12.49734 0.6213934 While this is all working quite nicely, the data provided to t.test() in the example above was in the form of two vectors. The data type we have been working with most during this course is a data frame. For example, the Gapminder and Stroop data are in the form of a data frame. We need to wrangle a bit with our data frames before t.test() will work with it. I will briefly demonstrate how to use t.test() with data frames in wide and long format in the examples below. First, with a wide format: # For this example, we put classic and modern in a data frame as follows: df_grades &lt;- data.frame(classic, modern) # This data frame is in a wide format: head(df_grades) ## classic modern ## 1 66.07667 83.56857 ## 2 68.38876 77.51870 ## 3 80.91096 77.80540 ## 4 70.49356 75.77478 ## 5 70.90501 71.10911 ## 6 82.00545 87.50839 # We can use t.test() with a data frame in wide format as follows: results &lt;- t.test(df_grades$classic, df_grades$modern) print(results) ## ## Welch Two Sample t-test ## ## data: df_grades$classic and df_grades$modern ## t = -1.9029, df = 17.872, p-value = 0.07329 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -12.4973418 0.6213934 ## sample estimates: ## mean of x mean of y ## 70.52238 76.46035 So far, so good. What if our data frame has a long format? library(tidyverse) # Pivot our data frame from wide to long: df_grades_long &lt;- df_grades %&gt;% pivot_longer(cols = c(classic,modern), names_to = &quot;Instruction&quot;, values_to = &quot;Grade&quot;) # This data frame is in a long format: head(df_grades_long) ## # A tibble: 6 x 2 ## Instruction Grade ## &lt;chr&gt; &lt;dbl&gt; ## 1 classic 66.1 ## 2 modern 83.6 ## 3 classic 68.4 ## 4 modern 77.5 ## 5 classic 80.9 ## 6 modern 77.8 We can use t.test() with a data frame in long format in two ways. The first way is not the preferred way of doing it, but I will show it for the sake of completeness. It goes as follows: # Select data of the classic instruction group classic_group &lt;- df_grades_long %&gt;% filter(Instruction == &quot;classic&quot;) %&gt;% select(Grade) # Select data of the modern instruction group modern_group &lt;- df_grades_long %&gt;% filter(Instruction == &quot;modern&quot;) %&gt;% select(Grade) # Do the t-test results &lt;- t.test(classic_group$Grade, modern_group$Grade) print(results) ## ## Welch Two Sample t-test ## ## data: classic_group$Grade and modern_group$Grade ## t = -1.9029, df = 17.872, p-value = 0.07329 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -12.4973418 0.6213934 ## sample estimates: ## mean of x mean of y ## 70.52238 76.46035 The preferred way is actually using the formula and data arguments of t.test(): results &lt;- t.test(formula = Grade ~ Instruction, data = df_grades_long) print(results) ## ## Welch Two Sample t-test ## ## data: Grade by Instruction ## t = -1.9029, df = 17.872, p-value = 0.07329 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -12.4973418 0.6213934 ## sample estimates: ## mean in group classic mean in group modern ## 70.52238 76.46035 This way of doing a \\(t\\)-test is especially relevant for you when working with the Gapminder data in your assignment, which is a data frame in long format. 6.4 Stroop data Let’s have another look at the Stroop experiment data from last week. The Stroop data should still be saved in your lab folder, so load the data as we did during the last lab: library(tidyverse) library(readxl) # read_xlsx depends on where you have placed and how you have named your file df_stroop_wide &lt;- read_xlsx(&quot;data/stroop.xlsx&quot;) df_stroop_long &lt;- pivot_longer(df_stroop_wide,cols = c(Congruent,Incongruent),names_to = &quot;Condition&quot;,values_to = &quot;RT&quot;) If you don’t have a copy of last week’s data saved, you can download an example file from Canvas under the lecture for Week 6. 6.4.1 \\(t\\)-test exercises A colleague hypothesizes that the average reaction time for the Congruent condition is 1000ms in the population (\\(\\mu = 1000\\)). Suppose you want to test if there is a statistically significant difference between the mean reaction time in the Congruent condition of your sample and the mean of the population. State the null and alternative hypothesis for this test. Conduct the appropriate statistical test to test these hypotheses. What do you conclude about the null hypothesis? We continue working with the Stroop data from last week, but this time we are interested in testing if there is a statistically significant difference in reaction time between the Congruent and Incongruent condition. We strongly expect a positive difference and only want to test positive Stroop effects (i.e. Incogruent - Congruent &gt; 0). Complete the following exercises: State the null and alternative hypothesis for this test. Conduct the appropriate statistical test to test these hypotheses. What do you conclude about the null hypothesis? Finally, we are going to finish this lab by do something “illegal.” Don’t worry, you aren’t going to get arrested, but I do want you to understand the gravity of the statistical crime we are going to commit. It’s for a good reason, so we are allowing it for once… We know the data in the Stroop experiment was collected in pairs: you took part in both the Congruent and Incongruent condition. However, how to analyze the Stroop data if we disregard this fact and pretend the data was collected in independent groups instead? So, let’s pretend, for didactic purposes, the reaction times for the Congruent condition were collected in one group of students and the Incongruent condition in a different group of students. We want to test if there is a difference between the two groups. State the null and alternative hypothesis for this test. Conduct the appropriate statistical test to test these hypotheses. What do you conclude about the null hypothesis? When you have completed all exercises and are happy with your progress today, please knit your document and submit it to Canvas. If you finish before the time is up, think about what topics from this course you would like to revise, work on your assignment, or help out your fellow students. Case from Introductory Statistics by OpenStax↩︎ Cases from Introductory Statistics by OpenStax↩︎ "],["function-list.html", " 7 Function list 7.1 Week 1 7.2 Week 2 7.3 Week 3 7.4 Week 4 7.5 Week 5 7.6 Week 6", " 7 Function list 7.1 Week 1 Function Explanation Section install.packages() Installs a package. Typically used once to install new packages. 0.4.5 library() Loads an installed package. Packages should be loaded with every new R session and inside your RMarkdown documents to ensure proper knitting. 0.4.5 +, -, *, /, ^ Basic operators used for arithmetic in R. The order of operations is BEDMAS. 1.5.1.1 sqrt() Computes the square root of a number. 1.5.2 abs() Calculates the absolute value of a number. 1.5.2 round() Round a number to a specified amount of digits. 1.5.2.1 log() Computes the (natural) logarithm of a number. 1.5.2.4 rep() Replicates elements (numbers, vectors, lists) a specified amount of times. 1.5.2.4 print() Print the value of a variable to the console. 1.5.4 seq() Generate a sequence of numbers from a starting value to an end value by a set increment. 1.5.3.3 # Indicates the start of a comment in R code. Everything after # is ignored by R. Comments enable you to document your R code with human readable text. 1.5.4 7.2 Week 2 Function Explanation Section General functions read_csv() Load a .csv file into R as a data frame. 2.2 head() Print the first 6 rows of a data frame to the console. 2.2.1 tail() Print the last 6 rows of a data frame to the console. 2.2.1 glimpse() Print a brief summary of a data frame to the console. 2.2.1 length() Calculate the length of a vector. 2.3.1 and 2.5.1.2 sum() Calculate the sum of all values in a vector. 2.5.1.1 c() Create a vector (combine elements). A vector is a variable that holds multiple elements. 2.4.2.3 data.frame() Create a data frame from variables. 2.5.4 Data wrangling $ Extract a single column/variable from a data frame. 2.2.1 %&gt;% The pipe operator allows you to chain multiple operations on a data frame (without saving intermediates). The pipe operator can be read as “and then.” 2.3.1 filter() Keep only those rows of a data frame that satisfy a certain condition, usually based on values in a specific column. 2.4.2.2 group_by() Divide a data frame into groups based on the values in a given column. If given two column names, groups based on the first column are divided into subgroups based on the second column. Operations on grouped data frames are performed by (sub)group. 2.3.1 and 2.5.4.1 summarise() Create a data frame with summary statistics. Combine with group_by() to calculate summary statistics for groups and subgroups. 2.3.1 and 2.5.4.1 %in% Match values in a vector. Can be used to filter a data frame for multiple values. 2.4.2.3 Visualization ggplot() Create a plot of the data in a data frame. You can create various kinds of plots with a wide range of aesthetics based on the specification of ggplot layers. Layers are added by using the + operator. 2.3.2 and 2.3.3 knittr::kable() Apply to a data frame to print out a nice looking table in your knitted document. 2.5.4.1 Descriptive statistics mean() Calculate the mean of a vector. 2.5.2.1 median() Calculate the median of a vector. 2.5.2.2 range() Calculate the minimum and maximum values of a vector. 2.5.3.1 var() Calculate the variance of a vector. Variance is the sum of squares divided by \\(n-1\\). 2.5.3.2 sd() Calculate the standard deviation of a vector. This is equal to sqrt(var). 2.5.3.3 7.3 Week 3 Function Explanation Section cor() Calculate the correlation between 2 vectors. 3.3.1 annotate() ggplot layer to annotate a plot with text and/or symbols 3.3.1.1 rnorm() Generate a vector of random numbers drawn from a normal distribution. Part of the *norm family of functions for the normal distribution. See the textbook for an explanation of the different distribution functions. 3.3.1.2 facet_wrap() Divide your plot into multiple facets based on a condition/variable. 3.3.1.2 select() Reduce a data frame to only the given columns 3.4.3 7.4 Week 4 Function Explanation Section for (i in x:y) { } Programmatically loop over the code between { } (i.e. evaluate the code multiple times) from a start index (x) to end index (y). 4.2.1 sample() Take a random sample of the elements in a vector. 4.3.1 rbinom() Generate a vector of random numbers drawn from a binomial distribution. Part of the *binom family of functions for the binomial distribution. See the textbook for an explanation of the different distribution functions. 4.3.2 set.seed() Set the state of the random number generator in R. Typically used to get the same random sampling between R sessions. 4.4.4 7.5 Week 5 Function Explanation Section replicate() Evaluate an expression multiple times. Analogous to rep() but for functions/expressions instead of vectors. 5.3.7 pivot_longer() Pivot data from wide format to long format. Tidyverse functions (like ggplot) expect long format data. 5.4 pivot_wider() Pivot data from long format to wide format. 5.4 mutate() Create a new variable from an existing one. 5.4 n() Calculate the number of observations in a group. 5.4 7.6 Week 6 Function Explanation Section t.test() Performs one and two sample t-tests on vectors of data. 6.3 "],["references.html", " 8 References", " 8 References R Core Team. 2020. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. "]]
